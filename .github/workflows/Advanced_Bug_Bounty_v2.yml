name: Advanced Bug Bounty Scanner

on:
  push:
    branches: [main]
  pull_request:
    branches: [main]
  workflow_dispatch:
  schedule:
    - cron: '13 1 * * *' # Every day at 01:13 UTC
    - cron: '47 5 * * *' # Every day at 05:47 UTC
    - cron: '29 9 * * *' # Every day at 09:29 UTC
    - cron: '53 14 * * *' # Every day at 14:53 UTC
    - cron: '11 18 * * *' # Every day at 18:11 UTC

jobs:
  vulnerability-scan:
    runs-on: ubuntu-latest
    timeout-minutes: 180 # Increased timeout to 3 hours for comprehensive scans

    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Cache Go modules
        uses: actions/cache@v4
        with:
          path: ~/go
          key: ${{ runner.os }}-go-${{ hashFiles('go.mod') }} # Caches Go modules, adjust if you have a go.mod
          restore-keys: |
            ${{ runner.os }}-go-

      - name: Cache apt packages
        uses: actions/cache@v4
        with:
          path: /var/cache/apt/archives
          key: ${{ runner.os }}-apt-${{ hashFiles('.github/workflows/Advanced_Bug_Bounty_Scanner.yml') }} # Invalidates cache if workflow dependencies change
          restore-keys: |
            ${{ runner.os }}-apt-

      - name: Install scanning dependencies
        run: |
          # Enable command tracing for detailed debugging output
          set -x
          # Exit immediately if a command exits with a non-zero status
          set -e

          sudo apt-get update
          sudo apt-get install -y git python3 python3-pip nikto nmap unzip wget golang jq curl

          # Set Go environment variables
          echo "GOPATH=$HOME/go" >> $GITHUB_ENV
          echo "PATH=$PATH:/usr/local/go/bin:$HOME/go/bin" >> $GITHUB_ENV

          # Create Go bin directory if it doesn't exist, though go install usually does
          mkdir -p "$HOME/go/bin"

          echo "Installing Go tools..."
          go install github.com/tomnomnom/assetfinder@latest
          go install github.com/projectdiscovery/nuclei/v3/cmd/nuclei@latest
          go install github.com/projectdiscovery/httpx/cmd/httpx@latest
          go install github.com/projectdiscovery/subfinder/v2/cmd/subfinder@latest
          go install github.com/lc/gau/v2/cmd/gau@latest
          go install github.com/projectdiscovery/katana/cmd/katana@latest
          go install github.com/sensepost/gowitness@latest # For web screenshots

          echo "Contents of Go bin directory before moving:"
          ls -l "$HOME/go/bin/" || true

          echo "Moving Go binaries to /usr/local/bin/"
          for tool in assetfinder nuclei httpx subfinder gau katana gowitness; do
            if [ -f "$HOME/go/bin/$tool" ]; then
              sudo mv "$HOME/go/bin/$tool" /usr/local/bin/
              echo "Moved $tool successfully."
            else
              echo "ERROR: Go tool '$tool' not found in $HOME/go/bin/. It might not have installed correctly."
              exit 1
            fi
          done

          # --- Start github-endpoints installation (more robust method) ---
          echo "Installing github-endpoints directly via pip from Git repository..."
          if pip3 install git+https://github.com/initstring/github-endpoints.git; then
            echo "github-endpoints installed successfully."
          else
            echo "ERROR: Failed to install github-endpoints via pip. Cannot proceed."
            exit 1
          fi
          # --- End github-endpoints installation ---

          # Install Findomain
          echo "Installing Findomain..."
          curl -s https://api.github.com/repos/findomain/findomain/releases/latest \
            | grep "browser_download_url.*linux.zip" \
            | cut -d '"' -f 4 \
            | wget -i - || { echo "ERROR: Failed to download Findomain zip URL."; exit 1; }
          unzip findomain-linux.zip || { echo "ERROR: Failed to unzip Findomain."; exit 1; }
          chmod +x findomain
          sudo mv findomain /usr/local/bin/findomain || { echo "ERROR: Failed to move Findomain to /usr/local/bin."; exit 1; }
          echo "Findomain installed successfully."

          # Install dirsearch
          echo "Installing dirsearch..."
          git clone https://github.com/dirsearch/dirsearch.git /opt/dirsearch
          sudo ln -s /opt/dirsearch/dirsearch.py /usr/local/bin/dirsearch
          echo "dirsearch installed successfully."

          # Update Nuclei templates to the latest version
          echo "Updating Nuclei templates..."
          nuclei -update-templates -silent || { echo "WARNING: Nuclei template update failed."; }
          echo "Nuclei templates updated."

      - name: Log in to GitHub Container Registry
        run: echo ${{ secrets.GITHUB_TOKEN }} | docker login ghcr.io -u ${{ github.actor }} --password-stdin

      - name: Prepare Initial Target List (domains.txt / organizations.txt)
        id: prepare-targets
        run: |
          # Verify that at least one target file exists
          if [ ! -f ".github/workflows/domains.txt" ] && [ ! -f ".github/workflows/organizations.txt" ]; then
            echo "ERROR: Neither .github/workflows/domains.txt nor .github/workflows/organizations.txt file found! Exiting."
            exit 1
          fi

          # Create a temporary file to store all discovered domains for this run
          discovered_domains_file="temp_discovered_domains.txt"
          touch "$discovered_domains_file"

          # Add domains from the static domains.txt file if it exists
          if [ -f ".github/workflows/domains.txt" ]; then
            echo "Loading initial target domains from .github/workflows/domains.txt:"
            cat .github/workflows/domains.txt >> "$discovered_domains_file"
            sort -u "$discovered_domains_file" -o "$discovered_domains_file" # Deduplicate
            sed -i '/^$/d' "$discovered_domains_file" # Remove empty lines
            echo "Current merged list after domains.txt:"
            cat "$discovered_domains_file"
          fi

          # Perform dynamic scope discovery from organizations.txt if it exists
          if [ -f ".github/workflows/organizations.txt" ]; then
            echo "Starting dynamic discovery from .github/workflows/organizations.txt..."
            while IFS= read -r org; do
              org=$(echo "$org" | tr -d '\r\n')
              if [ -z "$org" ]; then continue; fi # Skip empty lines

              echo "üîç Discovering domains for organization: $org"

              # Use github-endpoints to find domains in public GitHub repositories
              echo "Running github-endpoints for $org..."
              github-endpoints -o "$org" -m "domains" | httpx -silent >> "$discovered_domains_file" || true

              # Use assetfinder for broad passive domain discovery (includes crt.sh, etc.)
              echo "Running assetfinder for $org..."
              assetfinder --subs-only "$org" | httpx -silent >> "$discovered_domains_file" || true

            done < .github/workflows/organizations.txt
          fi

          # Final deduplication and cleanup of the combined domain list
          sort -u "$discovered_domains_file" -o "$discovered_domains_file"
          sed -i '/^$/d' "$discovered_domains_file"

          echo "Final list of domains for subdomain enumeration and scanning:"
          cat "$discovered_domains_file"

          # Overwrite domains.txt with the comprehensive list for subsequent steps and next runs
          mv "$discovered_domains_file" .github/workflows/domains.txt

      - name: Detect subdomains (with multi-domain diffing)
        id: subdomain-detection
        run: |
          set -e # Exit immediately if a command exits with a non-zero status.
          mkdir -p subdomain_output old_subdomains
          new_subdomains_found=()

          while IFS= read -r domain; do
            domain=$(echo "$domain" | tr -d '\r\n')
            if [ -z "$domain" ]; then continue; fi # Skip empty lines

            echo "üîé Processing $domain"
            output_file="subdomain_output/${domain//./_}_subdomains.txt"
            touch "$output_file" # Ensure output file exists for appending

            # Run multiple subdomain enumeration tools and probe for live hosts
            assetfinder --subs-only "$domain" | httpx -silent >> "$output_file" || true
            subfinder -d "$domain" -silent >> "$output_file" || true
            findomain -t "$domain" --quiet >> "$output_file" || true
            gau "$domain" | grep "^\(http\|https\)" | httpx -silent >> "$output_file" || true # Passive URLs and subdomains

            sort -u "$output_file" -o "$output_file" # Sort and unique the results
            sed -i '/^$/d' "$output_file" # Remove any lingering empty lines

            old_file="old_subdomains/${domain//./_}_old_subdomains.txt"
            if [ ! -f "$old_file" ]; then
              echo "üåü First scan for $domain ‚Äî storing results"
              cp "$output_file" "$old_file"
              new_subdomains_found+=("$domain")
            else
              if ! diff -q "$output_file" "$old_file" > /dev/null; then
                echo "üî• New subdomains detected for $domain! Triggering deeper scans."
                new_subdomains_found+=("$domain")
                cp "$output_file" "$old_file" # Update old results with new ones
              else
                echo "‚úÖ No new subdomains for $domain"
              fi
            fi
          done < .github/workflows/domains.txt # Read from the consolidated domains.txt

          echo "NEW_DOMAINS=${new_subdomains_found[*]}" >> $GITHUB_ENV
          if [ "${#new_subdomains_found[@]}" -gt 0 ]; then
            echo "new_subdomains_detected=true" >> $GITHUB_ENV
          else
            echo "new_subdomains_detected=false" >> $GITHUB_ENV
          fi

      - name: Take Web Screenshots
        if: env.new_subdomains_detected == 'true'
        run: |
          mkdir -p screenshots
          for domain in $NEW_DOMAINS; do
            echo "üì∏ Taking screenshots for $domain"
            subdomains_file="subdomain_output/${domain//./_}_subdomains.txt"
            if [ -f "$subdomains_file" ]; then
              # Use gowitness to screenshot all discovered live subdomains
              gowitness file -f "$subdomains_file" -o "screenshots/${domain//./_}" || true
            else
              echo "No active subdomains file found for $domain to screenshot. Skipping."
            fi
          done

      - name: Run Nikto Scan
        if: env.new_subdomains_detected == 'true'
        run: |
          mkdir -p nikto-reports
          for domain in $NEW_DOMAINS; do
            echo "üî¨ Nikto scanning $domain"
            # Nikto can be verbose; output to HTML and allow non-zero exit for no findings.
            nikto -h "https://$domain" -Format htm -output "nikto-reports/nikto-${domain//./_}.html" -nointeractive || echo "Nikto scan completed for $domain (no critical issues or general exit)"
          done

      - name: Run Dirsearch Scan
        if: env.new_subdomains_detected == 'true'
        run: |
          mkdir -p dirsearch-reports
          for domain in $NEW_DOMAINS; do
            echo "üìÇ Running dirsearch on $domain"
            subdomains_file="subdomain_output/${domain//./_}_subdomains.txt"
            if [ -f "$subdomains_file" ]; then
              # Iterate through live subdomains and run dirsearch against each
              # Using -f for full URLs, -o for output, --simple-report for clean text output
              cat "$subdomains_file" | while read -r url; do
                if [[ "$url" == http* ]]; then # Only scan actual HTTP(S) URLs
                  echo "  Scanning: $url"
                  # Run dirsearch.py with common wordlists, increased threads, and custom output
                  python3 /opt/dirsearch/dirsearch.py -u "$url" -w /opt/dirsearch/db/dicc.txt -t 20 --plain-text-report="dirsearch-reports/dirsearch-${url//[^a-zA-Z0-9]/_}.txt" || true
                fi
              done
            else
              echo "No active subdomains file found for $domain for Dirsearch scan. Skipping."
            fi
          done

      - name: Run Nuclei Scan (Targeted and Deep)
        if: env.new_subdomains_detected == 'true'
        run: |
          mkdir -p nuclei-reports
          for domain in $NEW_DOMAINS; do
            echo "üí£ Nuclei scanning $domain"
            subdomains_file="subdomain_output/${domain//./_}_subdomains.txt"
            if [ -f "$subdomains_file" ]; then
              # Active crawling with katana to get more URLs within the domain
              echo "Crawling $domain with katana for deeper Nuclei scan..."
              katana -u "https://$domain" -silent -d 3 -jc -o "katana_urls_for_${domain//./_}.txt" || true
              
              # Combine live subdomains and crawled URLs for Nuclei input
              cat "$subdomains_file" "katana_urls_for_${domain//./_}.txt" | sort -u > "all_targets_for_nuclei_${domain//./_}.txt"
              
              # Run Nuclei on the combined list of URLs/domains
              nuclei -l "all_targets_for_nuclei_${domain//./_}.txt" -severity critical,high,medium,low,info -j -o "nuclei-reports/nuclei-${domain//./_}.json" -timeout 60 -silent || echo "Nuclei scan completed for $domain"

              # Clean up temporary files
              rm -f "katana_urls_for_${domain//./_}.txt" "all_targets_for_nuclei_${domain//./_}.txt"
            else
              echo "No active subdomains file found for $domain for Nuclei scan. Skipping."
            fi
          done

      - name: Generate Summary Reports
        if: env.new_subdomains_detected == 'true'
        run: |
          mkdir -p summaries
          echo "domain,severity,name,url" > summaries/nuclei-findings.csv
          for file in nuclei-reports/*.json; do
            if [ -s "$file" ]; then # Check if file is not empty
              jq -r '[.host, .info.severity, .info.name, .matched] | @csv' "$file" >> summaries/nuclei-findings.csv
            fi
          done

          echo "<html><body><h1>Scan Summary</h1>" > summaries/nuclei-summary.html
          echo "<h2>Nuclei Findings (Critical/High/Medium/Low/Info)</h2>" >> summaries/nuclei-summary.html
          echo "<table border='1'><tr><th>Domain</th><th>Severity</th><th>Name</th><th>URL</th></tr>" >> summaries/nuclei-summary.html
          if [ -s summaries/nuclei-findings.csv ]; then # Check if CSV has data
            tail -n +2 summaries/nuclei-findings.csv | while IFS=',' read -r domain severity name url; do
              echo "<tr><td>$domain</td><td>$severity</td><td>$name</td><td><a href='$url'>$url</a></td></tr>" >> summaries/nuclei-summary.html
            done
          else
            echo "<tr><td colspan='4'>No Nuclei findings reported.</td></tr>" >> summaries/nuclei-summary.html
          fi
          echo "</table>" >> summaries/nuclei-summary.html

          echo "<h2>Nikto Scan Reports (HTML)</h2>" >> summaries/nikto-summary.html
          echo "<ul>" >> summaries/nikto-summary.html
          for file in nikto-reports/*.html; do
              if [ -s "$file" ]; then # Check if file is not empty
                  filename=$(basename "$file")
                  echo "<li><a href='../nikto-reports/$filename'>$filename</a></li>" >> summaries/nikto-summary.html
              fi
          done
          echo "</ul>" >> summaries/nikto-summary.html

          echo "<h2>Dirsearch Reports (Plain Text)</h2>" >> summaries/dirsearch-summary.html
          echo "<ul>" >> summaries/dirsearch-summary.html
          for file in dirsearch-reports/*.txt; do
              if [ -s "$file" ]; then # Check if file is not empty
                  filename=$(basename "$file")
                  echo "<li><a href='../dirsearch-reports/$filename'>$filename</a></li>" >> summaries/dirsearch-summary.html
              fi
          done
          echo "</ul>" >> summaries/dirsearch-summary.html


          echo "<h2>Web Screenshots</h2>" >> summaries/screenshots-summary.html
          echo "<ul>" >> summaries/screenshots-summary.html
          # Find all PNG files in the screenshots directory and link them
          find screenshots -name "*.png" | sed 's|^screenshots/||' | while read -r img_path; do
              echo "<li><a href='../screenshots/$img_path'>$img_path</a></li>" >> summaries/screenshots-summary.html
          done
          echo "</ul>" >> summaries/screenshots-summary.html

          echo "</body></html>" >> summaries/nuclei-summary.html


      - name: Upload Artifacts
        uses: actions/upload-artifact@v4
        if: env.new_subdomains_detected == 'true'
        with:
          name: scan-results-${{ github.run_id }}
          path: |
            summaries/*
            nikto-reports/*
            nuclei-reports/*
            dirsearch-reports/*
            screenshots/*
            subdomain_output/* # Upload subdomain lists for review
            old_subdomains/* # Upload old subdomain lists for state persistence if needed externally

      - name: Create GitHub Issues for Critical/High
        if: env.new_subdomains_detected == 'true'
        env:
          GH_TOKEN: ${{ secrets.GITHUB_TOKEN }}
        run: |
          for file in nuclei-reports/*.json; do
            if [ -s "$file" ]; then # Process only if the Nuclei report file is not empty
              jq -c 'select(.info.severity == "critical" or .info.severity == "high")' "$file" | while read -r vuln; do
                title=$(echo "$vuln" | jq -r '.info.name')
                url=$(echo "$vuln" | jq -r '.matched')
                severity=$(echo "$vuln" | jq -r '.info.severity')
                template=$(echo "$vuln" | jq -r '.template') # Include template ID for better context
                body="**Severity**: $severity%0A**URL**: $url%0A**Template**: $template%0A**Found in Workflow Run**: https://github.com/${{ github.repository }}/actions/runs/${{ github.run_id }}"
                
                # Basic deduplication: Check for an existing open issue with the same title
                # This is a simple check; for robust deduplication, you might need to track findings in a database.
                existing_issue=$(gh issue list --search "is:open in:title \"$title [$severity]\"" --json number --jq '.[0].number' || true)
                if [ -z "$existing_issue" ]; then
                  echo "Creating GitHub issue for: $title [$severity]"
                  gh issue create --title "$title [$severity]" --body "$body" --label "bug,bounty,auto-created,${severity}"
                else
                  echo "Skipping duplicate issue for: $title [$severity] (Issue #$existing_issue already open)"
                fi
              done
            fi
          done

      - name: Notify Slack of Critical/High Findings
        if: env.new_subdomains_detected == 'true' && success() # Run only if new subdomains were detected and previous steps succeeded
        run: |
          # Check if the SLACK_WEBHOOK_URL secret is provided.
          # We check it here in the `run` block because `secrets` cannot be directly accessed in `if:` conditionals.
          if [ -z "${{ secrets.SLACK_WEBHOOK_URL }}" ]; then
            echo "SLACK_WEBHOOK_URL secret is not configured. Skipping Slack notifications."
            exit 0 # Exit this step successfully if the secret isn't there
          fi

          NOTIFICATIONS_SENT=false
          for file in nuclei-reports/*.json; do
            if [ -s "$file" ]; then # Process only if the Nuclei report file is not empty
              jq -c 'select(.info.severity == "critical" or .info.severity == "high")' "$file" | while read -r vuln; do
                domain=$(echo "$vuln" | jq -r '.host' | sed 's|https://||;s|http://||;s|/.*||') # Extract base domain
                title=$(echo "$vuln" | jq -r '.info.name')
                url=$(echo "$vuln" | jq -r '.matched')
                severity=$(echo "$vuln" | jq -r '.info.severity')
                template=$(echo "$vuln" | jq -r '.template') # Include template ID

                SLACK_MESSAGE="{\"text\":\"‚ö†Ô∏è *Critical/High vulnerability detected!*\n*Domain:* $domain\n*Severity:* $severity\n*Name:* $title\n*URL:* <$url|$url>\n*Template:* $template\n*Workflow Run:* https://github.com/${{ github.repository }}/actions/runs/${{ github.run_id }}\"}"

                echo "Sending Slack notification for: $title ($severity) on $domain"
                curl -X POST -H 'Content-type: application/json' \
                  --data "$SLACK_MESSAGE" \
                  "${{ secrets.SLACK_WEBHOOK_URL }}" || true # Use || true to prevent job failure if Slack API call fails
                NOTIFICATIONS_SENT=true
              done
            fi
          done

          if [ "$NOTIFICATIONS_SENT" = false ]; then
            echo "No Critical/High findings to notify Slack about for this run."
          fi
