name: Production JS Security Scanner

on:
  push:
    branches: [main, master]
    paths:
      - 'targets.txt'
  workflow_dispatch:
  schedule:
    - cron: '0 */6 * * *'

permissions:
  contents: write
  actions: read

jobs:
  intelligent-scan:
    runs-on: ubuntu-latest
    timeout-minutes: 45

    steps:
      - name: Checkout Code
        uses: actions/checkout@v4
        with:
          fetch-depth: 0

      - name: Setup Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.11'

      - name: Setup Directories
        run: |
          mkdir -p reports tmp stats
          echo "âœ… Workspace ready"

      - name: Cache Tools
        id: cache-tools
        uses: actions/cache@v4
        with:
          path: ~/go/bin
          key: ${{ runner.os }}-tools-v5

      - name: Install Dependencies
        run: |
          echo "ğŸ“¦ Installing packages..."
          pip install --no-cache-dir groq aiohttp requests || pip install --break-system-packages groq aiohttp requests

          if [[ ! -f ~/go/bin/katana ]]; then
            go install github.com/projectdiscovery/katana/cmd/katana@latest
          fi

          echo "$(go env GOPATH)/bin" >> $GITHUB_PATH
          echo "âœ… Ready"

      - name: Create Production Scanner
        run: |
          cat > production_js_scanner.py << 'PYTHON_SCRIPT'
          #!/usr/bin/env python3
          """
          Production-Grade JS Security Scanner
          v3 â€” addresses Gemini critique:
            1. Parallelized downloads + analysis (asyncio semaphore, configurable concurrency)
            2. Strict "secret" pattern â€” requires value to be outside an enum/constant context
            3. Known public SDK key registry (Branch.io key_live_, etc.)
            4. Context classifier â€” rejects enum/constant/route/UI string contexts
            5. LLM "public by default" stance for frontend JS
          """

          import os
          import sys
          import json
          import asyncio
          import re
          import hashlib
          import math
          import subprocess
          from typing import List, Dict, Optional, Set, Tuple
          from urllib.parse import urlparse

          try:
              from groq import Groq
              GROQ_AVAILABLE = True
          except ImportError:
              GROQ_AVAILABLE = False

          # â”€â”€ Init â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
          GROQ_API_KEY = os.getenv("GROQ_API_KEY")
          client = Groq(api_key=GROQ_API_KEY) if GROQ_API_KEY and GROQ_AVAILABLE else None

          # Parallelism â€” 8 concurrent downloads, 4 concurrent LLM calls
          DOWNLOAD_CONCURRENCY = 8
          LLM_CONCURRENCY = 4

          # â”€â”€ Patterns â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
          # Each pattern requires the VALUE to be meaningfully secret.
          # "secret" and "api_key" now require stricter surrounding context.
          PATTERNS = {
              "private_key": {
                  # PEM block â€” structure is the proof
                  "regex": r'-----BEGIN (RSA |EC |DSA |OPENSSH )?PRIVATE KEY-----',
                  "severity": "CRITICAL",
                  "risk": "Full system compromise",
                  "require_entropy": False,
              },
              "aws_access_key": {
                  # AWS key IDs have a fixed 20-char structure
                  "regex": r'(?<![A-Z0-9])(AKIA|ASIA|AROA|AIDA|ANPA|ANVA|APKA)[A-Z0-9]{16}(?![A-Z0-9])',
                  "severity": "CRITICAL",
                  "risk": "AWS account takeover",
                  "require_entropy": False,
              },
              "github_token": {
                  "regex": r'gh[pousr]_[A-Za-z0-9_]{36,}',
                  "severity": "CRITICAL",
                  "risk": "Repository access",
                  "require_entropy": False,
              },
              "slack_token": {
                  "regex": r'xox[baprs]-[0-9]{10,13}-[0-9]{10,13}-[a-zA-Z0-9]{24,}',
                  "severity": "HIGH",
                  "risk": "Slack workspace compromise",
                  "require_entropy": False,
              },
              "jwt_token": {
                  # Only flag JWTs that look like they could be real session tokens
                  # (not test/demo JWTs in documentation)
                  "regex": r'eyJ[A-Za-z0-9_-]{20,}\.[A-Za-z0-9_-]{20,}\.[A-Za-z0-9_-]{20,}',
                  "severity": "HIGH",
                  "risk": "Session hijacking â€” live token may be reusable",
                  "require_entropy": False,
              },
              "database_url": {
                  "regex": r'(?i)(mongodb(\+srv)?|mysql|postgresql|postgres|redis|amqp)://[^@\s\'"<>]{3,}@[^\s\'"<>]{4,}',
                  "severity": "CRITICAL",
                  "risk": "Direct database access with credentials",
                  "require_entropy": False,
              },
              "api_key_assignment": {
                  # Requires: left side is apiKey/api_key, right side is a quoted
                  # value that is NOT a known public-key prefix and has high entropy.
                  # Explicitly excludes enum-style assignment like e.AccountSecurity=...
                  "regex": r'(?i)\b(api[_\-]?key|apikey|access[_\-]?key)\s*[:=]\s*["\']([a-zA-Z0-9_\-\.]{20,})["\']',
                  "severity": "HIGH",
                  "risk": "Unauthorized API access",
                  "require_entropy": True,
                  "value_group": 2,
              },
              "secret_assignment": {
                  # Tighter than before: only match when the VALUE looks like a real
                  # secret (not a route string, UI label, or code constant).
                  # Requires high entropy on the value AND excludes lines that look
                  # like enum/constant assignments (e.g. e.AccountSecuritySuccess=...).
                  "regex": r'(?i)(?<!\w)(client[_\-]?secret|app[_\-]?secret|oauth[_\-]?secret|signing[_\-]?secret)\s*[:=]\s*["\']([^"\']{16,})["\']',
                  "severity": "HIGH",
                  "risk": "Authentication bypass / OAuth compromise",
                  "require_entropy": True,
                  "value_group": 2,
              },
          }

          # â”€â”€ Public SDK key prefixes â€” intentionally embedded in frontend JS â”€â”€â”€â”€â”€â”€
          # Adding these here means they pass the regex but get dropped before flagging.
          SAFE_KEY_PREFIXES = [
              # Mobile / web SDKs (public client keys)
              "key_live_", "key_test_",          # Branch.io
              "pk_live_", "pk_test_",            # Stripe publishable
              "wg_",                             # Weglot
              "AIza",                            # Google Maps / Firebase (public)
              "G-", "UA-", "GTM-", "AW-",       # Google Analytics / Ads
              "ca-pub-",                         # Google AdSense
              "BQ",                              # Firebase (public config)
              # Common placeholder / non-secret patterns
              "undefined", "null", "false", "true",
          ]

          # â”€â”€ Third-party domains â€” always skip â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
          THIRD_PARTY_DOMAINS = [
              "googletagmanager.com", "google-analytics.com", "analytics.google.com",
              "doubleclick.net", "googlesyndication.com", "googleadservices.com",
              "facebook.net", "connect.facebook.net", "fbcdn.net",
              "cdn.segment.com", "cdn.weglot.com",
              "hotjar.com", "clarity.ms",
              "intercom.io", "intercomcdn.com",
              "zendesk.com", "zdassets.com",
              "hubspot.com", "hs-scripts.com",
              "stripe.com", "js.stripe.com",
              "cdn.jsdelivr.net", "unpkg.com", "cdnjs.cloudflare.com",
              "ads.lyft.com", "ads.twitter.com", "static.ads-twitter.com",
              "branch.io", "app.link",
          ]

          # â”€â”€ Library filename indicators â€” skip these JS files entirely â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
          LIBRARY_INDICATORS = [
              "jquery", "bootstrap", "angular", "lodash", "moment",
              "webpack", "vendor", "polyfill", "chunk", "runtime",
              "analytics", "gtag", "recaptcha", "turnstile", "captcha",
              "sentry", "datadog", "newrelic", "rollbar",
              "matomo", "piwik", "onetrust", "otSDK", "cookiebot",
              "intercom", "freshchat", "drift", "crisp",
          ]

          # â”€â”€ Context patterns that indicate a match is NOT a real secret â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
          # If surrounding context matches these, it's an enum, route, or UI string.
          FALSE_POSITIVE_CONTEXT_PATTERNS = [
              # Enum-style: e.SomethingSecret = "some_event_name"
              r'\be\.[A-Z][a-zA-Z]+(Secret|Password|Key|Token)[A-Z]',
              # Route or event name strings like "account_security_change_password"
              r'"[a-z_]{5,}(secret|password|key|token)[a-z_]{0,}"',
              # JSDoc / comment context
              r'(//|/\*|\*)\s.*(secret|password|key)',
              # Enum value assignment pattern: identifier = "string_constant"
              r'\.[A-Z][a-zA-Z]{3,}\s*=\s*"[a-z_]{5,}"',
          ]

          # â”€â”€ Boilerplate value indicators â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
          BOILERPLATE_VALUES = [
              "your_", "insert_", "replace_", "enter_", "paste_", "add_your",
              "your-key", "your_key", "your_token", "changeme", "change_me",
              "fill_in", "key_here", "token_here", "secret_here", "api_key_here",
              "replace_me", "todo", "fixme", "<your", "xxxxxxxx", "aaaaaaaa",
          ]

          # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

          def shannon_entropy(data: str) -> float:
              if not data:
                  return 0.0
              freq: Dict[str, int] = {}
              for ch in data:
                  freq[ch] = freq.get(ch, 0) + 1
              n = len(data)
              return -sum((c / n) * math.log2(c / n) for c in freq.values())

          def extract_value(match_text: str, group: int = 0) -> str:
              """Extract the secret value portion from a regex match."""
              # Try quoted value after = or :
              m = re.search(r'[:=]\s*["\']([^"\']{8,})["\']', match_text)
              return m.group(1) if m else match_text

          def is_boilerplate_value(value: str) -> bool:
              v = value.lower()
              return any(bp in v for bp in BOILERPLATE_VALUES)

          def is_safe_public_key(value: str) -> bool:
              return any(value.startswith(pfx) for pfx in SAFE_KEY_PREFIXES)

          def is_false_positive_context(context: str) -> bool:
              """Return True if context looks like an enum, route, or UI constant."""
              for pattern in FALSE_POSITIVE_CONTEXT_PATTERNS:
                  if re.search(pattern, context):
                      return True
              return False

          def has_sufficient_entropy(value: str, threshold: float = 3.5) -> bool:
              if len(value) < 16:
                  return False
              return shannon_entropy(value) >= threshold


          # â”€â”€ Downloader â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

          import concurrent.futures
          _thread_pool = concurrent.futures.ThreadPoolExecutor(max_workers=12)

          def _curl_fetch(url: str, timeout: int) -> Optional[str]:
              """Blocking curl fetch â€” run in thread pool for concurrency."""
              try:
                  import subprocess as _sp
                  result = _sp.run(
                      ["curl", "-sL",
                       "-A", "Mozilla/5.0 (compatible; SecurityScanner/2.0)",
                       "--max-time", str(timeout),
                       "--max-filesize", "4000000",
                       "--retry", "1",
                       "-H", "Accept: application/javascript, text/javascript, */*",
                       url],
                      capture_output=True,
                      timeout=timeout + 5,
                  )
                  if result.returncode != 0:
                      return None
                  content = result.stdout.decode("utf-8", errors="ignore")
                  if len(content) < 200:
                      return None
                  # Reject HTML responses (error pages / redirects)
                  stripped = content.lstrip()
                  if (stripped.startswith("<!") or
                          stripped.lower().startswith("<html") or
                          re.match(r'<\s*!DOCTYPE', stripped, re.IGNORECASE)):
                      return None
                  return content
              except Exception:
                  return None

          async def download_js(url: str, sem: asyncio.Semaphore, timeout: int = 15) -> Optional[str]:
              """Async wrapper: runs blocking curl in a thread pool."""
              async with sem:
                  loop = asyncio.get_event_loop()
                  return await loop.run_in_executor(_thread_pool, _curl_fetch, url, timeout)


          # â”€â”€ Analyzer â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

          class JavaScriptAnalyzer:
              def __init__(self, target_domain: str):
                  self.target_domain = target_domain.lower().lstrip("www.")
                  self.seen_hashes: Set[str] = set()
                  self.llm_sem = asyncio.Semaphore(LLM_CONCURRENCY)
                  self.stats: Dict[str, int] = {
                      "total_files": 0,
                      "skipped_third_party": 0,
                      "skipped_library": 0,
                      "skipped_html": 0,
                      "skipped_duplicate": 0,
                      "analyzed": 0,
                      "findings": 0,
                      "fp_suppressed": 0,
                  }

              # â”€â”€ URL filters â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

              def is_same_domain(self, url: str) -> bool:
                  host = urlparse(url).netloc.lower().lstrip("www.")
                  return host == self.target_domain or host.endswith("." + self.target_domain)

              def is_known_third_party(self, url: str) -> bool:
                  host = urlparse(url).netloc.lower()
                  return any(tp in host for tp in THIRD_PARTY_DOMAINS)

              def is_library_file(self, url: str) -> bool:
                  path = urlparse(url).path.lower()
                  return any(lib in path for lib in LIBRARY_INDICATORS)

              # â”€â”€ Content filters â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

              def is_html(self, content: str) -> bool:
                  s = content.lstrip()
                  return (s.startswith("<!") or s.lower().startswith("<html") or
                          bool(re.match(r'<\s*!DOCTYPE', s, re.IGNORECASE)))

              def is_duplicate(self, content: str) -> bool:
                  h = hashlib.md5(content.encode()).hexdigest()
                  if h in self.seen_hashes:
                      return True
                  self.seen_hashes.add(h)
                  return False

              # â”€â”€ Context extractor â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

              def get_context(self, content: str, pos: int, size: int = 200) -> str:
                  start = max(0, pos - size)
                  end = min(len(content), pos + size)
                  return re.sub(r'\s+', ' ', content[start:end]).strip()

              # â”€â”€ Pattern scan â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

              def pattern_scan(self, content: str, url: str) -> Tuple[List[Dict], int]:
                  findings: List[Dict] = []
                  suppressed = 0

                  for name, info in PATTERNS.items():
                      try:
                          for m in re.finditer(info["regex"], content):
                              raw_match = m.group(0)
                              # Extract the actual secret value
                              vg = info.get("value_group", 0)
                              value = m.group(vg) if vg and vg <= len(m.groups()) else extract_value(raw_match)
                              context = self.get_context(content, m.start())

                              # â”€â”€ FP gates (ordered cheapest first) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
                              # 1. Boilerplate placeholder value
                              if is_boilerplate_value(value):
                                  suppressed += 1
                                  continue

                              # 2. Known public SDK key
                              if is_safe_public_key(value):
                                  suppressed += 1
                                  continue

                              # 3. Context looks like enum / route / UI constant
                              if is_false_positive_context(context):
                                  suppressed += 1
                                  continue

                              # 4. Entropy gate (only for patterns that need it)
                              if info.get("require_entropy", False):
                                  if not has_sufficient_entropy(value, threshold=3.5):
                                      suppressed += 1
                                      continue

                              # 5. Value is suspiciously short
                              if len(value) < 12:
                                  suppressed += 1
                                  continue
                              # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

                              findings.append({
                                  "type": name,
                                  "severity": info["severity"],
                                  "risk": info["risk"],
                                  "url": url,
                                  "source_domain": urlparse(url).netloc,
                                  "context": context[:300],
                                  "match_preview": raw_match[:80],
                                  "line": content[:m.start()].count("\n") + 1,
                                  "confidence": "pattern_match",
                                  "entropy": round(shannon_entropy(value), 2),
                                  "recommendation": self._recommendation(name),
                              })
                      except Exception:
                          continue

                  return findings, suppressed

              def _recommendation(self, vuln_type: str) -> str:
                  return {
                      "private_key":        "URGENT: Rotate immediately. Audit all systems that used this key.",
                      "aws_access_key":     "URGENT: Deactivate in AWS IAM. Enable CloudTrail audit immediately.",
                      "github_token":       "Revoke on GitHub. Check audit log for unauthorised access.",
                      "slack_token":        "Revoke in Slack workspace settings. Review channel audit log.",
                      "jwt_token":          "Invalidate all sessions using this signing key.",
                      "database_url":       "URGENT: Rotate DB credentials. Restrict access by IP/VPC.",
                      "api_key_assignment": "Rotate key. Move to server-side env vars or secrets manager.",
                      "secret_assignment":  "Rotate secret. Move to server-side env vars or secrets manager.",
                  }.get(vuln_type, "Review and rotate if this is a real credential.")

              # â”€â”€ LLM analysis â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

              async def llm_analyze(self, content: str, url: str) -> Optional[Dict]:
                  """
                  LLM pass â€” 'public by default' stance.
                  Only called on files that already have suspicious lines.
                  Gemini critique: LLM needs stricter rubric.
                  """
                  if not client:
                      return None

                  # Only pull lines that contain credential-like keywords
                  keywords = ["AKIA", "private_key", "client_secret", "oauth_secret",
                              "signing_secret", "access_token", "refresh_token",
                              "mongodb://", "redis://", "postgres://"]
                  suspicious = [l for l in content.split("\n")
                                if any(kw.lower() in l.lower() for kw in keywords)]

                  if len(suspicious) < 1:
                      return None

                  sample = "\n".join(suspicious[:25])

                  prompt = f"""You are a senior AppSec engineer doing a bug bounty review.
          Analyze this JavaScript snippet for REAL exposed backend secrets.

          Target: {urlparse(url).netloc}
          File: {os.path.basename(url)}

          ```javascript
          {sample[:1800]}
          ```

          GROUND RULES â€” assume "public by default" for frontend JS:
          - Client-side SDK keys (Branch key_live_, Stripe pk_live_, Google AIza*, G-*,
            Firebase, Segment write key, Mixpanel token) are PUBLIC. Do NOT flag these.
          - UI state constants, event name enums, route strings are NOT secrets.
          - Empty strings, process.env references, template variables are NOT secrets.
          - Only flag if you see a value that: (a) is a backend secret/private key,
            (b) has high randomness (entropy), AND (c) should NOT be in frontend code.

          Examples of TRUE positives: AWS secret access key, GitHub PAT, Slack bot token,
          database connection string with real credentials, OAuth client_secret.

          Examples of FALSE positives (do NOT flag): Google Analytics G-XXXXX,
          Branch.io key_live_XXXXX, Stripe pk_live_XXXXX, any enum like
          e.AccountSecuritySuccess, any route string, any UI label.

          If you find a REAL backend secret:
          {{
            "has_vulnerability": true,
            "type": "aws_secret_key|github_token|database_url|oauth_client_secret|other",
            "severity": "CRITICAL|HIGH",
            "evidence": "the specific line (partially redact the value)",
            "reasoning": "why this is definitely a real secret",
            "confidence": "high"
          }}

          If nothing real: {{"has_vulnerability": false}}

          Return ONLY valid JSON. No markdown."""

                  async with self.llm_sem:
                      try:
                          resp = client.chat.completions.create(
                              model="llama-3.3-70b-versatile",
                              messages=[{"role": "user", "content": prompt}],
                              max_tokens=500,
                              temperature=0.05,
                          )
                          raw = resp.choices[0].message.content.strip()
                          raw = raw.replace("```json", "").replace("```", "").strip()
                          result = json.loads(raw)

                          if (result.get("has_vulnerability") and
                                  result.get("confidence") == "high"):
                              return {
                                  "type": result.get("type", "unknown"),
                                  "severity": result.get("severity", "HIGH"),
                                  "risk": "Backend secret in frontend JS",
                                  "url": url,
                                  "source_domain": urlparse(url).netloc,
                                  "context": result.get("evidence", "")[:300],
                                  "reasoning": result.get("reasoning", "")[:200],
                                  "confidence": "llm_verified",
                                  "recommendation": "Rotate immediately. Investigate how this reached frontend code.",
                              }
                      except Exception as e:
                          print(f"  LLM error: {e}", file=sys.stderr)

                  return None

              # â”€â”€ Main per-file entry â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

              async def analyze_file(self, url: str, content: str) -> List[Dict]:
                  self.stats["total_files"] += 1

                  if not self.is_same_domain(url) or self.is_known_third_party(url):
                      self.stats["skipped_third_party"] += 1
                      return []
                  if self.is_library_file(url):
                      self.stats["skipped_library"] += 1
                      return []
                  if self.is_html(content):
                      self.stats["skipped_html"] += 1
                      return []
                  if self.is_duplicate(content):
                      self.stats["skipped_duplicate"] += 1
                      return []

                  self.stats["analyzed"] += 1

                  pattern_findings, suppressed = self.pattern_scan(content, url)
                  self.stats["fp_suppressed"] += suppressed

                  # LLM pass only when suspicious keywords exist
                  llm_finding = await self.llm_analyze(content, url)
                  if llm_finding:
                      existing = {f["type"] for f in pattern_findings}
                      if llm_finding["type"] not in existing:
                          pattern_findings.append(llm_finding)

                  self.stats["findings"] += len(pattern_findings)
                  return pattern_findings


          # â”€â”€ Main â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

          async def main() -> None:
              if len(sys.argv) < 3:
                  print("Usage: scanner.py <js_urls.txt> <target_domain>", file=sys.stderr)
                  sys.exit(1)

              js_list, target_domain = sys.argv[1], sys.argv[2]

              if not os.path.exists(js_list):
                  print(f"âŒ File not found: {js_list}", file=sys.stderr)
                  sys.exit(1)

              with open(js_list) as f:
                  urls = [l.strip() for l in f if l.strip()]

              if not urls:
                  print(json.dumps({"findings": [], "stats": {}}))
                  return

              analyzer = JavaScriptAnalyzer(target_domain)
              dl_sem = asyncio.Semaphore(DOWNLOAD_CONCURRENCY)

              print(f"ğŸ§  JS Security Scanner v3", file=sys.stderr)
              print(f"   Target : {target_domain}", file=sys.stderr)
              print(f"   URLs   : {len(urls)}", file=sys.stderr)
              print(f"   Concur : {DOWNLOAD_CONCURRENCY} downloads / {LLM_CONCURRENCY} LLM", file=sys.stderr)
              print(f"   LLM    : {'âœ… Active' if client else 'âš ï¸  Pattern-only'}", file=sys.stderr)
              print("", file=sys.stderr)

              # â”€â”€ Parallel download â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
              async def fetch_and_analyze(i: int, url: str) -> List[Dict]:
                  fname = os.path.basename(urlparse(url).path) or "unknown.js"
                  print(f"[{i:02d}/{len(urls)}] â†“ {fname[:50]}", file=sys.stderr)
                  content = await download_js(url, dl_sem)
                  if not content:
                      print(f"[{i:02d}]    âš ï¸  skip (no content / HTML)", file=sys.stderr)
                      return []
                  findings = await analyzer.analyze_file(url, content)
                  status = f"ğŸ”¥ {len(findings)} finding(s)" if findings else "âœ“ clean"
                  print(f"[{i:02d}]    {status}", file=sys.stderr)
                  return findings

              tasks = [fetch_and_analyze(i, url) for i, url in enumerate(urls, 1)]
              results = await asyncio.gather(*tasks)
              all_findings = [f for batch in results for f in batch]

              # Sort: CRITICAL first
              sev = {"CRITICAL": 0, "HIGH": 1, "MEDIUM": 2, "LOW": 3}
              all_findings.sort(key=lambda x: sev.get(x.get("severity", "LOW"), 4))

              output = {"findings": all_findings, "stats": analyzer.stats}

              # Write JSON to file (bash wrapper reads this file)
              with open("scan_results.json", "w") as f:
                  json.dump(output, f, indent=2)

              print("", file=sys.stderr)
              print("ğŸ“Š Stats:", file=sys.stderr)
              for k, v in analyzer.stats.items():
                  print(f"   {k:25s}: {v}", file=sys.stderr)

          if __name__ == "__main__":
              asyncio.run(main())
          PYTHON_SCRIPT

          chmod +x production_js_scanner.py

      - name: Run Production Scan
        env:
          GROQ_API_KEY: ${{ secrets.GROQ_API_KEY }}
        run: |
          set -euo pipefail

          mkdir -p reports tmp stats

          domains_scanned=0
          js_analyzed=0
          total_findings=0
          critical_findings=0
          high_findings=0

          if [[ ! -f targets.txt ]] || [[ ! -s targets.txt ]]; then
            echo "example.com" > targets.txt
          fi

          grep -v '^#' targets.txt | grep -v '^$' | shuf -n 6 > tmp/selected.txt || echo "example.com" > tmp/selected.txt

          echo "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•"
          echo "ğŸ§  PRODUCTION JS SECURITY SCANNER"
          echo "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•"
          cat tmp/selected.txt | nl
          echo "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•"
          echo ""

          while read -r domain; do
            [[ -z "$domain" ]] && continue
            domain=$(echo "$domain" | tr -d '[:space:]' | tr '[:upper:]' '[:lower:]')
            [[ -z "$domain" ]] && continue

            domains_scanned=$((domains_scanned + 1))

            echo ""
            echo "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•"
            echo "ğŸ” [$domains_scanned] $domain"
            echo "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•"

            report="reports/${domain}_report.md"

            echo "ğŸ“¡ Discovering JavaScript..."
            timeout 90s katana -u "https://$domain" -jc -d 3 -c 20 -silent \
              > tmp/urls_${domain}.txt 2>/dev/null || touch tmp/urls_${domain}.txt

            if [[ -s tmp/urls_${domain}.txt ]]; then

              # â”€â”€ Domain-scoped JS filtering â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
              base_domain="${domain#www.}"   # strip leading www. if present
              total_urls=$(wc -l < tmp/urls_${domain}.txt 2>/dev/null || echo "0")

              grep -iE '\.js(\?[^[:space:]]*)?$' tmp/urls_${domain}.txt \
                | grep -iv '\.json' \
                | grep -i "${base_domain}" \
                | grep -ivE '(%[0-9a-f]{2}){2,}' \
                | grep -ivE '(matomo|piwik|gtag|analytics|hotjar|clarity|segment|
intercom|zendesk|hubspot|cdn\.jsdelivr|unpkg\.com|cdnjs\.cloudflare)' \
                > tmp/js_${domain}_raw.txt 2>/dev/null || true

              # Deduplicate by basename so matomo.js x17 becomes matomo.js x1,
              # then take the 25 URLs with the most unique filenames.
              # Strategy: sort unique full URLs, then pick one per unique basename.
              sort -u tmp/js_${domain}_raw.txt | awk -F/ '!seen[$NF]++' \
                > tmp/js_${domain}_sorted.txt 2>/dev/null || true
              head -n 25 tmp/js_${domain}_sorted.txt > tmp/js_${domain}.txt 2>/dev/null || true
              rm -f tmp/js_${domain}_raw.txt tmp/js_${domain}_sorted.txt

              echo "   ğŸ“‹ katana: $total_urls URLs â†’ $(wc -l < tmp/js_${domain}.txt 2>/dev/null || echo 0) unique JS files queued"
              # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

              js_count=$(wc -l < tmp/js_${domain}.txt 2>/dev/null || echo "0")

              if [[ "$js_count" -gt 0 ]]; then
                echo "ğŸ§  Analyzing $js_count JavaScript files (domain-scoped)..."

                # Redirect ONLY stderr to log file â€” stdout must stay free so the
                # Python script can write scan_results.json (it uses json.dump to
                # file, not stdout). Redirecting stdout was silently swallowing output.
                python3 production_js_scanner.py tmp/js_${domain}.txt "$domain" \
                  2>tmp/scanner_log_${domain}.txt || true

                # Show scanner progress in workflow log
                cat tmp/scanner_log_${domain}.txt || true

                if [[ -f scan_results.json ]]; then
                  mv scan_results.json tmp/scan_${domain}.json

                  finding_count=$(jq '.findings | length' tmp/scan_${domain}.json 2>/dev/null || echo "0")
                  critical=$(jq '[.findings[] | select(.severity=="CRITICAL")] | length' tmp/scan_${domain}.json 2>/dev/null || echo "0")
                  high=$(jq '[.findings[] | select(.severity=="HIGH")] | length' tmp/scan_${domain}.json 2>/dev/null || echo "0")
                  analyzed=$(jq '.stats.analyzed' tmp/scan_${domain}.json 2>/dev/null || echo "0")
                  skipped_tp=$(jq '.stats.skipped_third_party' tmp/scan_${domain}.json 2>/dev/null || echo "0")
                  skipped_html=$(jq '.stats.skipped_html' tmp/scan_${domain}.json 2>/dev/null || echo "0")
                  skipped_lib=$(jq '.stats.skipped_library' tmp/scan_${domain}.json 2>/dev/null || echo "0")
                  fp_suppressed=$(jq '.stats.fp_suppressed' tmp/scan_${domain}.json 2>/dev/null || echo "0")

                  total_findings=$((total_findings + finding_count))
                  critical_findings=$((critical_findings + critical))
                  high_findings=$((high_findings + high))
                  js_analyzed=$((js_analyzed + analyzed))

                  cat > "$report" << EOF
          # ğŸ§  JS Security Analysis: $domain

          **Scan Date:** $(date '+%Y-%m-%d %H:%M UTC')
          **Run:** #${{ github.run_number }}
          **Scanner:** v3 (parallel + FP-hardened)
          **Intelligence:** ${GROQ_API_KEY:+Groq LLM Active (public-by-default stance)}${GROQ_API_KEY:-Pattern-Only Mode}

          ---

          ## ğŸ“Š Scan Statistics

          | Metric | Value |
          |--------|------:|
          | JS Files Discovered | $js_count |
          | Files Analyzed | $analyzed |
          | Skipped (Third-Party) | $skipped_tp |
          | Skipped (Library) | $skipped_lib |
          | Skipped (HTML Response) | $skipped_html |
          | FP Matches Suppressed | $fp_suppressed |
          | Total Findings | **$finding_count** |
          | Critical Issues | **$critical** |
          | High Severity | **$high** |

          EOF

                  if [[ "$finding_count" -gt 0 ]]; then
                    echo "" >> "$report"
                    echo "## ğŸš¨ Security Findings" >> "$report"
                    echo "" >> "$report"

                    for severity in CRITICAL HIGH MEDIUM; do
                      count=$(jq "[.findings[] | select(.severity==\"$severity\")] | length" tmp/scan_${domain}.json 2>/dev/null || echo "0")
                      if [[ "$count" -gt 0 ]]; then
                        echo "### ${severity} Severity ($count)" >> "$report"
                        echo "" >> "$report"

                        jq -r ".findings[] | select(.severity==\"$severity\") | \"
          #### \(.type | gsub(\"_\"; \" \") | ascii_upcase)

          **Severity:** \(.severity)
          **Risk:** \(.risk)
          **File:** \`\(.url)\`
          **Source Domain:** \`\(.source_domain // \"unknown\")\`
          \(if .line then \"**Line:** ~\(.line)\" else \"\"\ end)
          **Confidence:** \(.confidence)\(if .entropy then \" | Entropy: \(.entropy)\" else \"\"\ end)
          \(if .reasoning then \"**LLM Reasoning:** \(.reasoning)\n\" else \"\"\ end)
          **Evidence:**
          \`\`\`javascript
          \(.context // .evidence // \"(see file)\")
          \`\`\`

          **âš¡ Action Required:**
          > \(.recommendation)

          ---
          \"" tmp/scan_${domain}.json >> "$report" 2>/dev/null
                      fi
                    done
                  else
                    echo "" >> "$report"
                    echo "## âœ… No Security Issues Detected" >> "$report"
                    echo "" >> "$report"
                    echo "All analyzed JavaScript files passed security checks." >> "$report"
                  fi

                  echo "âœ… Findings: $finding_count ($critical critical, $high high) | FP suppressed: $fp_suppressed | Skipped HTML: $skipped_html | Skipped 3rd-party: $skipped_tp | Skipped lib: $skipped_lib"
                else
                  echo "âš ï¸ Scanner failed"
                fi
              else
                echo "âš ï¸ No same-domain JavaScript found after filtering"
                cat > "$report" << EOF
          # ğŸ§  JS Security Analysis: $domain

          **Scan Date:** $(date '+%Y-%m-%d %H:%M UTC')
          **Run:** #${{ github.run_number }}

          ## â„¹ï¸ No JavaScript Found

          No same-domain JavaScript files were discovered for this target.
          All discovered JS was filtered as third-party or non-JS content.
          EOF
              fi
            else
              echo "âš ï¸ Discovery failed"
            fi

            sleep 1

          done < tmp/selected.txt

          echo ""
          echo "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•"
          echo "ğŸ“Š SCAN COMPLETE"
          echo "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•"
          echo "Domains:         $domains_scanned"
          echo "JS Files:        $js_analyzed"
          echo "Total Findings:  $total_findings"
          echo "  Critical:      $critical_findings"
          echo "  High:          $high_findings"
          echo "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•"

          cat > stats/scan_${{ github.run_number }}.json << STATS_EOF
          {
            "run": ${{ github.run_number }},
            "date": "$(date -u +%Y-%m-%dT%H:%M:%SZ)",
            "domains": $domains_scanned,
            "js_files": $js_analyzed,
            "findings": {
              "total": $total_findings,
              "critical": $critical_findings,
              "high": $high_findings
            }
          }
          STATS_EOF

          echo "DOMAINS_SCANNED=$domains_scanned"   >> $GITHUB_ENV
          echo "JS_ANALYZED=$js_analyzed"           >> $GITHUB_ENV
          echo "TOTAL_FINDINGS=$total_findings"     >> $GITHUB_ENV
          echo "CRITICAL_FINDINGS=$critical_findings" >> $GITHUB_ENV
          echo "HIGH_FINDINGS=$high_findings"       >> $GITHUB_ENV

          [[ $total_findings  -gt 0 ]] && echo "HAS_FINDINGS=true"  >> $GITHUB_ENV || echo "HAS_FINDINGS=false"  >> $GITHUB_ENV
          [[ $critical_findings -gt 0 ]] && echo "HAS_CRITICAL=true" >> $GITHUB_ENV || echo "HAS_CRITICAL=false" >> $GITHUB_ENV

      - name: Generate Executive Summary
        if: always()
        run: |
          mkdir -p reports

          cat > reports/EXECUTIVE_SUMMARY.md << 'SUMMARY_EOF'
          # ğŸ§  JS Security Scanner - Executive Summary

          **Scan Date:** $(date '+%Y-%m-%d %H:%M UTC')
          **Run:** #${{ github.run_number }}
          **Intelligence:** ${GROQ_API_KEY:+âœ… AI-Powered}${GROQ_API_KEY:-âš ï¸ Pattern-Only}

          ---

          ## ğŸ“Š Key Metrics

          | Metric | Value | Status |
          |--------|------:|:------:|
          | Domains Scanned | ${DOMAINS_SCANNED:-0} | âœ… |
          | JS Files Analyzed | ${JS_ANALYZED:-0} | âœ… |
          | **Total Findings** | **${TOTAL_FINDINGS:-0}** | $(if [[ "${TOTAL_FINDINGS:-0}" -gt 0 ]]; then echo "âš ï¸"; else echo "âœ…"; fi) |
          | Critical Issues | ${CRITICAL_FINDINGS:-0} | $(if [[ "${CRITICAL_FINDINGS:-0}" -gt 0 ]]; then echo "ğŸš¨"; else echo "âœ…"; fi) |
          | High Severity | ${HIGH_FINDINGS:-0} | $(if [[ "${HIGH_FINDINGS:-0}" -gt 0 ]]; then echo "âš ï¸"; else echo "âœ…"; fi) |

          ## ğŸ¯ Scanned Domains

          $(cat tmp/selected.txt 2>/dev/null | nl -w2 -s'. ' || echo "None")

          ## ğŸ” Assessment

          $(if [[ "${CRITICAL_FINDINGS:-0}" -gt 0 ]]; then
            echo "### ğŸš¨ CRITICAL ALERT"
            echo ""
            echo "${CRITICAL_FINDINGS} critical security issue(s) detected requiring **immediate action**."
            echo ""
            echo "**Recommended Actions:**"
            echo "1. Review detailed reports immediately"
            echo "2. Rotate any exposed credentials"
            echo "3. Check access logs for unauthorized use"
            echo "4. Implement remediation steps"
          elif [[ "${HIGH_FINDINGS:-0}" -gt 0 ]]; then
            echo "### âš ï¸ HIGH PRIORITY"
            echo ""
            echo "${HIGH_FINDINGS} high-severity issue(s) detected requiring **prompt attention**."
            echo ""
            echo "**Recommended Actions:**"
            echo "1. Review findings within 24 hours"
            echo "2. Assess impact and exposure"
            echo "3. Plan remediation timeline"
          elif [[ "${TOTAL_FINDINGS:-0}" -gt 0 ]]; then
            echo "### â„¹ï¸ FINDINGS DETECTED"
            echo ""
            echo "${TOTAL_FINDINGS} medium/low severity issue(s) detected."
            echo ""
            echo "**Recommended Actions:**"
            echo "1. Review at next scheduled security review"
            echo "2. Add to remediation backlog"
          else
            echo "### âœ… ALL CLEAR"
            echo ""
            echo "No security issues detected in analyzed JavaScript files."
            echo ""
            echo "**Next Steps:**"
            echo "- Continue regular scanning"
            echo "- Monitor for new deployments"
          fi)

          ## ğŸ“ Detailed Reports

          $(ls reports/*_report.md 2>/dev/null | sed 's/reports\//- /' || echo "- No domain reports")

          ---

          **Repository:** ${{ github.repository }}
          **Workflow:** https://github.com/${{ github.repository }}/actions/runs/${{ github.run_id }}
          SUMMARY_EOF

      - name: Upload Results
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: security-scan-${{ github.run_number }}
          path: |
            reports/
            stats/
            tmp/*.json
          retention-days: 30

      - name: Email Security Report
        if: always() && env.DOMAINS_SCANNED != '0'
        continue-on-error: true
        uses: dawidd6/action-send-mail@v3
        with:
          server_address: smtp.gmail.com
          server_port: 465
          username: ${{ secrets.EMAIL_USERNAME }}
          password: ${{ secrets.EMAIL_PASSWORD }}
          subject: "${{ env.HAS_CRITICAL == 'true' && 'ğŸš¨ CRITICAL' || env.HAS_FINDINGS == 'true' && 'âš ï¸ FINDINGS' || 'âœ… CLEAN' }} | JS Security Scan #${{ github.run_number }}"
          to: ${{ secrets.EMAIL_USERNAME }}
          from: Security Scanner <security@scanner.local>
          body: |
            ğŸ§  PRODUCTION JS SECURITY SCAN REPORT
            â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

            ğŸ“… $(date '+%Y-%m-%d %H:%M UTC')
            ğŸ”„ Run #${{ github.run_number }}

            ğŸ“Š METRICS
            â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
            â€¢ Domains:         ${{ env.DOMAINS_SCANNED }}
            â€¢ JS Files:        ${{ env.JS_ANALYZED }}
            â€¢ Total Findings:  ${{ env.TOTAL_FINDINGS }}
            â€¢ ğŸ”¥ Critical:     ${{ env.CRITICAL_FINDINGS }}
            â€¢ âš ï¸  High:        ${{ env.HIGH_FINDINGS }}

            ${{ env.HAS_CRITICAL == 'true' && 'ğŸš¨ğŸš¨ğŸš¨ CRITICAL ISSUES - IMMEDIATE ACTION REQUIRED ğŸš¨ğŸš¨ğŸš¨' || env.HAS_FINDINGS == 'true' && 'âš ï¸ SECURITY ISSUES DETECTED - REVIEW REQUIRED' || 'âœ… NO ISSUES DETECTED' }}

            ğŸ“ ATTACHMENTS
            â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
            â€¢ EXECUTIVE_SUMMARY.md - Overview
            â€¢ *_report.md - Detailed findings per domain

            ğŸ”— Full results:
            https://github.com/${{ github.repository }}/actions/runs/${{ github.run_id }}

            â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
            Powered by AI + Pattern Analysis | ${{ github.repository }}
          attachments: reports/*
          priority: ${{ env.HAS_CRITICAL == 'true' && 'high' || 'normal' }}

      - name: Cleanup
        if: always()
        run: rm -rf tmp/*.txt tmp/*.js 2>/dev/null || true
