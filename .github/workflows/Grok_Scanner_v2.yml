name: Production JS Security Scanner

on:
  push:
    branches: [main, master]
    paths:
      - 'targets.txt'
  workflow_dispatch:
  schedule:
    - cron: '0 */6 * * *'

permissions:
  contents: write
  actions: read

jobs:
  intelligent-scan:
    runs-on: ubuntu-latest
    timeout-minutes: 45

    steps:
      - name: Checkout Code
        uses: actions/checkout@v4
        with:
          fetch-depth: 0

      - name: Setup Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.11'

      - name: Setup Directories
        run: |
          mkdir -p reports tmp stats
          echo "âœ… Workspace ready"

      - name: Cache Tools
        id: cache-tools
        uses: actions/cache@v4
        with:
          path: ~/go/bin
          key: ${{ runner.os }}-tools-v5

      - name: Install Dependencies
        run: |
          echo "ğŸ“¦ Installing packages..."
          pip install --no-cache-dir groq aiohttp requests || pip install --break-system-packages groq aiohttp requests

          if [[ ! -f ~/go/bin/katana ]]; then
            go install github.com/projectdiscovery/katana/cmd/katana@latest
          fi

          echo "$(go env GOPATH)/bin" >> $GITHUB_PATH
          echo "âœ… Ready"

      - name: Create Production Scanner
        run: |
          cat > production_js_scanner.py << 'PYTHON_SCRIPT'
          #!/usr/bin/env python3
          """
          Production-Grade JS Security Scanner with Intelligence
          Fixes:
            - Domain scoping: only scan JS from the target domain
            - HTML response rejection: skip non-JS responses
            - False positive reduction: entropy checks, value validation,
              known-safe patterns, third-party key allowlists
          """

          import os
          import sys
          import json
          import asyncio
          import re
          import hashlib
          import math
          from typing import List, Dict, Optional, Set
          from urllib.parse import urlparse

          try:
              from groq import Groq
              GROQ_AVAILABLE = True
          except ImportError:
              GROQ_AVAILABLE = False

          # â”€â”€ Init â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
          GROQ_API_KEY = os.getenv("GROQ_API_KEY")
          client = Groq(api_key=GROQ_API_KEY) if GROQ_API_KEY and GROQ_AVAILABLE else None

          # â”€â”€ Patterns with severity â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
          PATTERNS = {
              "private_key": {
                  "regex": r'-----BEGIN (RSA |EC |DSA |OPENSSH )?PRIVATE KEY-----',
                  "severity": "CRITICAL",
                  "risk": "Full system compromise"
              },
              "aws_key": {
                  "regex": r'(?<![A-Z0-9])(AKIA|ASIA)[A-Z0-9]{16}(?![A-Z0-9])',
                  "severity": "CRITICAL",
                  "risk": "AWS account takeover"
              },
              "jwt_token": {
                  "regex": r'eyJ[A-Za-z0-9_-]{10,}\.[A-Za-z0-9_-]{10,}\.[A-Za-z0-9_-]{10,}',
                  "severity": "HIGH",
                  "risk": "Session hijacking"
              },
              "api_key": {
                  "regex": r'(?i)(api[_-]?key|apikey)\s*[:=]\s*["\']([a-zA-Z0-9_\-]{20,})["\']',
                  "severity": "HIGH",
                  "risk": "Unauthorized API access"
              },
              "github_token": {
                  "regex": r'gh[pousr]_[A-Za-z0-9_]{36,}',
                  "severity": "CRITICAL",
                  "risk": "Repository access"
              },
              "slack_token": {
                  "regex": r'xox[baprs]-[0-9]{10,13}-[0-9]{10,13}-[a-zA-Z0-9]{24,}',
                  "severity": "HIGH",
                  "risk": "Workspace compromise"
              },
              "secret": {
                  "regex": r'(?i)(secret|password)\s*[:=]\s*["\']([^"\']{12,})["\']',
                  "severity": "HIGH",
                  "risk": "Authentication bypass"
              },
              "database_url": {
                  "regex": r'(?i)(mongodb|mysql|postgres|redis)://[^\s\'"<>]{8,}',
                  "severity": "CRITICAL",
                  "risk": "Database access"
              },
              "bearer_token": {
                  "regex": r'(?i)bearer\s+[a-zA-Z0-9_\-\.]{20,}',
                  "severity": "HIGH",
                  "risk": "Unauthorized access"
              }
          }

          # â”€â”€ Boilerplate / placeholder indicators â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
          BOILERPLATE_INDICATORS = [
              "example", "demo", "test", "placeholder", "sample", "your_",
              "insert_", "replace_", "enter_", "paste_", "add_your",
              "TODO", "FIXME", "xxx", "yyy", "zzz", "abc123",
              "config.example", "env.example", ".env.sample",
              "<your", "your-key", "your_key", "your_token",
              "changeme", "change_me", "fill_in", "fill-in",
              "put_your", "put-your", "key_here", "token_here",
              "secret_here", "api_key_here", "replace_me"
          ]

          # â”€â”€ Known low-entropy / obviously fake value patterns â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
          FAKE_VALUE_PATTERNS = [
              r'^[a-zA-Z]{1,3}[0-9]{1,3}$',          # e.g. "ab1", "xyz99"
              r'^(true|false|null|undefined|none)$',
              r'^[0-9a-f]{1,8}$',                      # tiny hex strings
              r'^\$\{',                                  # template literals ${VAR}
              r'^\<',                                    # HTML tags
              r'^process\.env\.',                        # Node env refs
              r'^window\.',                              # browser globals
              r'^config\.',                              # config object refs
          ]

          # â”€â”€ Third-party / public keys that are intentionally client-side â”€â”€â”€â”€â”€â”€â”€â”€
          # These are PUBLIC keys designed to be embedded in JS â€” not secrets.
          SAFE_KEY_PREFIXES = [
              "pk_live_", "pk_test_",           # Stripe publishable keys
              "wg_",                             # Weglot public API keys
              "G-", "UA-", "GTM-",              # Google Analytics / Tag Manager
              "AIza",                            # Google API (public maps/analytics)
              "ca-pub-",                         # Google AdSense
              "APP_ID", "CLIENT_ID",             # generic public identifiers
          ]

          # These domain patterns host JS that commonly contains public/client keys
          THIRD_PARTY_DOMAINS = [
              "googletagmanager.com", "google-analytics.com", "analytics.google.com",
              "doubleclick.net", "googlesyndication.com",
              "facebook.net", "connect.facebook.net",
              "cdn.segment.com", "cdn.weglot.com",
              "hotjar.com", "clarity.ms",
              "intercom.io", "intercomcdn.com",
              "zendesk.com", "zdassets.com",
              "hubspot.com", "hs-scripts.com",
              "stripe.com", "js.stripe.com",
              "cdn.jsdelivr.net", "unpkg.com", "cdnjs.cloudflare.com",
              "ads.lyft.com", "ads.twitter.com",
          ]

          # â”€â”€ Library file indicators (skip entirely) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
          LIBRARY_INDICATORS = [
              "jquery", "bootstrap", "react", "angular", "vue", "lodash",
              "moment", "axios", "webpack", "bundle", "vendor", "polyfill",
              "chunk", "framework", "cdn", "min.js", "analytics", "gtag",
              "recaptcha", "turnstile", "captcha"
          ]

          # â”€â”€ Helpers â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

          def shannon_entropy(data: str) -> float:
              """Calculate Shannon entropy to detect high-randomness secrets."""
              if not data:
                  return 0.0
              freq = {}
              for ch in data:
                  freq[ch] = freq.get(ch, 0) + 1
              length = len(data)
              return -sum((c / length) * math.log2(c / length) for c in freq.values())


          def is_high_entropy(value: str, threshold: float = 3.5) -> bool:
              """Return True only if value looks like a real random secret."""
              return shannon_entropy(value) >= threshold


          def extract_secret_value(match_text: str) -> str:
              """Pull the raw value out of key=value or key: value patterns."""
              # Try to grab the quoted value portion
              m = re.search(r'[:=]\s*["\']([^"\']{8,})["\']', match_text)
              if m:
                  return m.group(1)
              return match_text


          class JavaScriptAnalyzer:
              def __init__(self, target_domain: str):
                  self.target_domain = target_domain
                  self.seen_hashes: Set[str] = set()
                  self.stats = {
                      "total_files": 0,
                      "skipped_duplicates": 0,
                      "skipped_libraries": 0,
                      "skipped_third_party": 0,
                      "skipped_html": 0,
                      "analyzed": 0,
                      "findings": 0
                  }

              # â”€â”€ URL-level filters â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

              def is_same_domain(self, url: str) -> bool:
                  """
                  FIX 1: Ensure the JS file is from the target domain (or a direct
                  subdomain). Rejects third-party scripts like ads.lyft.com when
                  scanning grafana.com.
                  """
                  try:
                      host = urlparse(url).netloc.lower().lstrip("www.")
                      base = self.target_domain.lower().lstrip("www.")
                      # allow exact match or subdomain (e.g. cdn.grafana.com)
                      return host == base or host.endswith("." + base)
                  except Exception:
                      return False

              def is_known_third_party(self, url: str) -> bool:
                  """Reject URLs from known analytics / ad / CDN domains."""
                  host = urlparse(url).netloc.lower()
                  return any(tp in host for tp in THIRD_PARTY_DOMAINS)

              def is_library(self, url: str) -> bool:
                  url_lower = url.lower()
                  return any(lib in url_lower for lib in LIBRARY_INDICATORS)

              # â”€â”€ Content-level filters â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

              def is_html_response(self, content: str) -> bool:
                  """
                  FIX 2: Reject HTML pages served instead of JS (redirects,
                  error pages, login walls).
                  """
                  stripped = content.lstrip()
                  return (
                      stripped.startswith("<!") or
                      stripped.lower().startswith("<html") or
                      bool(re.match(r'<\s*!DOCTYPE', stripped, re.IGNORECASE))
                  )

              def is_duplicate(self, content: str) -> bool:
                  h = hashlib.md5(content.encode()).hexdigest()
                  if h in self.seen_hashes:
                      return True
                  self.seen_hashes.add(h)
                  return False

              # â”€â”€ Finding-level filters â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

              def is_boilerplate(self, text: str) -> bool:
                  text_lower = text.lower()
                  if any(bp in text_lower for bp in BOILERPLATE_INDICATORS):
                      return True
                  if re.search(r'(x{5,}|y{5,}|z{5,}|\*{5,})', text_lower):
                      return True
                  if re.search(r'\b(12345|abcde|qwerty|foobar|bazqux)\b', text_lower):
                      return True
                  return False

              def is_safe_public_key(self, match_text: str) -> bool:
                  """
                  FIX 3: Skip keys that are intentionally public / client-side.
                  e.g. Stripe pk_live_, Weglot wg_, Google G-/UA- IDs.
                  """
                  value = extract_secret_value(match_text)
                  return any(value.startswith(pfx) for pfx in SAFE_KEY_PREFIXES)

              def is_fake_value(self, match_text: str) -> bool:
                  """
                  FIX 4: Reject values that match obviously-fake / template patterns.
                  """
                  value = extract_secret_value(match_text).strip()
                  for pattern in FAKE_VALUE_PATTERNS:
                      if re.match(pattern, value, re.IGNORECASE):
                          return True
                  return False

              def has_real_entropy(self, match_text: str, pattern_name: str) -> bool:
                  """
                  FIX 5: Entropy gate â€” only flag values with sufficient randomness.
                  Passwords / secrets often have low entropy if they're placeholders.
                  Skip entropy check for patterns where structure matters more (e.g. AWS keys).
                  """
                  # These patterns have structure-based validity â€” entropy not needed
                  structure_patterns = {"private_key", "aws_key", "github_token",
                                        "slack_token", "jwt_token", "database_url"}
                  if pattern_name in structure_patterns:
                      return True

                  value = extract_secret_value(match_text)
                  # Short values are almost always false positives
                  if len(value) < 16:
                      return False
                  return is_high_entropy(value, threshold=3.2)

              # â”€â”€ Core logic â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

              def extract_context(self, content: str, match_pos: int, size: int = 150) -> str:
                  start = max(0, match_pos - size)
                  end = min(len(content), match_pos + size)
                  context = content[start:end]
                  context = re.sub(r'\s+', ' ', context)
                  return context.strip()

              def get_recommendation(self, vuln_type: str) -> str:
                  recommendations = {
                      "private_key": "URGENT: Rotate key immediately. Never commit keys to code.",
                      "aws_key": "URGENT: Deactivate key in AWS Console. Enable CloudTrail to check for unauthorized access.",
                      "api_key": "Rotate key immediately. Move to environment variables or secrets manager.",
                      "github_token": "Revoke token on GitHub. Check audit log for unauthorized access.",
                      "slack_token": "Revoke token in Slack workspace. Review channel access logs.",
                      "database_url": "URGENT: Rotate credentials. Restrict database access by IP.",
                      "jwt_token": "If valid, invalidate all sessions. Implement proper token storage.",
                      "secret": "Rotate credential immediately. Move to secure secrets manager.",
                      "bearer_token": "Invalidate token. Implement proper authentication flow."
                  }
                  return recommendations.get(vuln_type, "Review and rotate if necessary.")

              def pattern_scan(self, content: str, url: str) -> List[Dict]:
                  findings = []

                  for pattern_name, pattern_info in PATTERNS.items():
                      try:
                          for match in re.finditer(pattern_info["regex"], content):
                              match_text = match.group(0)
                              context = self.extract_context(content, match.start())

                              # â”€â”€ False positive gates â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
                              if self.is_boilerplate(context):
                                  continue
                              if self.is_boilerplate(match_text):
                                  continue
                              if self.is_safe_public_key(match_text):
                                  continue
                              if self.is_fake_value(match_text):
                                  continue
                              if not self.has_real_entropy(match_text, pattern_name):
                                  continue
                              # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

                              findings.append({
                                  "type": pattern_name,
                                  "severity": pattern_info["severity"],
                                  "risk": pattern_info["risk"],
                                  "url": url,
                                  "source_domain": urlparse(url).netloc,
                                  "context": context[:250],
                                  "match": match_text[:100],
                                  "line": content[:match.start()].count('\n') + 1,
                                  "confidence": "pattern_match",
                                  "recommendation": self.get_recommendation(pattern_name)
                              })
                      except Exception:
                          continue

                  return findings

              async def llm_analyze(self, content: str, url: str) -> Optional[Dict]:
                  """LLM deep analysis for subtle issues â€” only on suspicious files."""
                  if not client:
                      return None

                  keywords = ["key", "secret", "token", "password", "auth",
                              "credential", "private", "bearer", "AKIA"]
                  suspicious_lines = [
                      line for line in content.split('\n')
                      if any(kw in line.lower() for kw in keywords)
                  ]

                  if len(suspicious_lines) < 2:
                      return None

                  sample = '\n'.join(suspicious_lines[:30])

                  prompt = f"""You are a strict security analyst. Analyze this JavaScript for REAL exposed secrets.

          File: {os.path.basename(url)}
          Domain: {urlparse(url).netloc}

          ```javascript
          {sample[:2000]}
          ```

          IMPORTANT RULES:
          - Only flag credentials with REAL values (high entropy, not placeholders).
          - SKIP: empty strings, template vars like ${VAR} or process.env.X, "YOUR_KEY_HERE".
          - SKIP: public/client-side keys (Stripe pk_live_, Google G-/UA-, Weglot wg_).
          - SKIP: anything that looks like documentation or example code.
          - SKIP: variable declarations with no assigned value.

          If you find a REAL secret, return ONLY this JSON:
          {{
            "has_vulnerability": true,
            "type": "specific_type",
            "severity": "CRITICAL|HIGH|MEDIUM",
            "description": "What was found",
            "evidence": "Exact line with issue (redact middle of value)",
            "confidence": "high|medium",
            "recommendation": "Specific action"
          }}

          If no real issues exist: {{"has_vulnerability": false}}

          Return ONLY valid JSON. No markdown, no explanation."""

                  try:
                      response = client.chat.completions.create(
                          model="llama-3.3-70b-versatile",
                          messages=[{"role": "user", "content": prompt}],
                          max_tokens=600,
                          temperature=0.1
                      )

                      raw = response.choices[0].message.content.strip()
                      raw = raw.replace("```json", "").replace("```", "").strip()
                      result = json.loads(raw)

                      if result.get("has_vulnerability") and result.get("confidence") == "high":
                          return {
                              "type": result.get("type", "unknown"),
                              "severity": result.get("severity", "MEDIUM"),
                              "risk": "Detected by LLM analysis",
                              "url": url,
                              "source_domain": urlparse(url).netloc,
                              "description": result.get("description", "")[:300],
                              "evidence": result.get("evidence", "")[:300],
                              "confidence": "llm_verified",
                              "recommendation": result.get("recommendation", "Review and remediate")
                          }

                  except Exception as e:
                      print(f"  LLM error: {e}", file=sys.stderr)

                  return None

              async def analyze_file(self, url: str, content: str) -> List[Dict]:
                  self.stats["total_files"] += 1

                  # â”€â”€ URL-level rejections â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
                  if not self.is_same_domain(url):
                      self.stats["skipped_third_party"] += 1
                      return []

                  if self.is_known_third_party(url):
                      self.stats["skipped_third_party"] += 1
                      return []

                  if self.is_library(url):
                      self.stats["skipped_libraries"] += 1
                      return []

                  # â”€â”€ Content-level rejections â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
                  if self.is_html_response(content):
                      self.stats["skipped_html"] += 1
                      return []

                  if self.is_duplicate(content):
                      self.stats["skipped_duplicates"] += 1
                      return []

                  self.stats["analyzed"] += 1
                  findings = []

                  # Pattern scan (fast)
                  pattern_findings = self.pattern_scan(content, url)
                  findings.extend(pattern_findings)

                  # LLM analysis (deeper, slower)
                  if client and len(content) > 500:
                      llm_finding = await self.llm_analyze(content, url)
                      if llm_finding:
                          # Avoid duplicate if LLM finds same thing as pattern
                          existing_types = {f["type"] for f in findings}
                          if llm_finding["type"] not in existing_types:
                              findings.append(llm_finding)

                  self.stats["findings"] += len(findings)
                  return findings


          # â”€â”€ Downloader â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

          async def download_js(url: str, timeout: int = 15) -> Optional[str]:
              """
              Download JS with content-type validation.
              FIX 2 (enforcement): Use -D to capture headers and reject HTML responses.
              """
              import subprocess
              try:
                  result = subprocess.run(
                      ["curl", "-sL", "-A", "Mozilla/5.0 (compatible; SecurityScanner/1.0)",
                       "--max-time", str(timeout),
                       "--max-filesize", "5000000",   # 5 MB cap
                       "-H", "Accept: application/javascript, text/javascript, */*",
                       url],
                      capture_output=True,
                      timeout=timeout + 2
                  )

                  if result.returncode != 0:
                      return None

                  content = result.stdout.decode('utf-8', errors='ignore')

                  # Reject empty or tiny responses
                  if len(content) < 200:
                      return None

                  # Reject HTML responses (FIX 2)
                  stripped = content.lstrip()
                  if (stripped.startswith("<!") or
                          stripped.lower().startswith("<html") or
                          re.match(r'<\s*!DOCTYPE', stripped, re.IGNORECASE)):
                      return None

                  return content

              except Exception:
                  return None


          # â”€â”€ Main â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

          async def main():
              if len(sys.argv) < 3:
                  print("Usage: python production_js_scanner.py <js_urls.txt> <target_domain>",
                        file=sys.stderr)
                  sys.exit(1)

              js_list = sys.argv[1]
              target_domain = sys.argv[2]

              if not os.path.exists(js_list):
                  print(f"âŒ Not found: {js_list}", file=sys.stderr)
                  sys.exit(1)

              with open(js_list, 'r') as f:
                  urls = [line.strip() for line in f if line.strip()]

              if not urls:
                  print(json.dumps({"findings": [], "stats": {}}))
                  return

              analyzer = JavaScriptAnalyzer(target_domain=target_domain)

              print(f"ğŸ§  Production Scanner", file=sys.stderr)
              print(f"   Target domain: {target_domain}", file=sys.stderr)
              print(f"   URLs: {len(urls)}", file=sys.stderr)
              print(f"   LLM: {'âœ… Active' if client else 'âš ï¸  Pattern-only'}", file=sys.stderr)
              print("", file=sys.stderr)

              all_findings = []

              for i, url in enumerate(urls, 1):
                  filename = os.path.basename(urlparse(url).path) or "unknown.js"
                  print(f"[{i}/{len(urls)}] {filename[:50]}", file=sys.stderr)

                  content = await download_js(url)

                  if not content:
                      print(f"  âš ï¸  Download failed / HTML response", file=sys.stderr)
                      continue

                  findings = await analyzer.analyze_file(url, content)

                  if findings:
                      print(f"  ğŸ”¥ {len(findings)} finding(s)", file=sys.stderr)
                      all_findings.extend(findings)
                  else:
                      print(f"  âœ“  Clean", file=sys.stderr)

                  await asyncio.sleep(0.5)

              # Sort by severity
              severity_order = {"CRITICAL": 0, "HIGH": 1, "MEDIUM": 2, "LOW": 3}
              all_findings.sort(key=lambda x: severity_order.get(x.get("severity", "LOW"), 4))

              output = {
                  "findings": all_findings,
                  "stats": analyzer.stats
              }

              print(json.dumps(output, indent=2))

              with open("scan_results.json", "w") as f:
                  json.dump(output, f, indent=2)

              print("", file=sys.stderr)
              print("ğŸ“Š Statistics:", file=sys.stderr)
              for k, v in analyzer.stats.items():
                  print(f"   {k}: {v}", file=sys.stderr)

          if __name__ == "__main__":
              asyncio.run(main())
          PYTHON_SCRIPT

          chmod +x production_js_scanner.py

      - name: Run Production Scan
        env:
          GROQ_API_KEY: ${{ secrets.GROQ_API_KEY }}
        run: |
          set -euo pipefail

          mkdir -p reports tmp stats

          domains_scanned=0
          js_analyzed=0
          total_findings=0
          critical_findings=0
          high_findings=0

          if [[ ! -f targets.txt ]] || [[ ! -s targets.txt ]]; then
            echo "example.com" > targets.txt
          fi

          grep -v '^#' targets.txt | grep -v '^$' | shuf -n 6 > tmp/selected.txt || echo "example.com" > tmp/selected.txt

          echo "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•"
          echo "ğŸ§  PRODUCTION JS SECURITY SCANNER"
          echo "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•"
          cat tmp/selected.txt | nl
          echo "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•"
          echo ""

          while read -r domain; do
            [[ -z "$domain" ]] && continue
            domain=$(echo "$domain" | tr -d '[:space:]' | tr '[:upper:]' '[:lower:]')
            [[ -z "$domain" ]] && continue

            domains_scanned=$((domains_scanned + 1))

            echo ""
            echo "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•"
            echo "ğŸ” [$domains_scanned] $domain"
            echo "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•"

            report="reports/${domain}_report.md"

            echo "ğŸ“¡ Discovering JavaScript..."
            timeout 90s katana -u "https://$domain" -jc -d 3 -c 20 -silent \
              > tmp/urls_${domain}.txt 2>/dev/null || touch tmp/urls_${domain}.txt

            if [[ -s tmp/urls_${domain}.txt ]]; then

              # â”€â”€ FIX 1: Filter to same domain only â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
              # Write to temp file first â€” avoids broken pipe error from sort|head
              # under set -euo pipefail
              grep -iE '\.js(\?|$)' tmp/urls_${domain}.txt \
                | grep -iv '\.json' \
                | grep -iE "(^https?://(www\.)?${domain}/|^https?://[a-z0-9-]+\.${domain}/)" \
                > tmp/js_${domain}_raw.txt 2>/dev/null || true

              sort -u tmp/js_${domain}_raw.txt > tmp/js_${domain}_sorted.txt 2>/dev/null || true
              head -n 20 tmp/js_${domain}_sorted.txt > tmp/js_${domain}.txt 2>/dev/null || true
              rm -f tmp/js_${domain}_raw.txt tmp/js_${domain}_sorted.txt
              # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

              js_count=$(wc -l < tmp/js_${domain}.txt 2>/dev/null || echo "0")

              if [[ "$js_count" -gt 0 ]]; then
                echo "ğŸ§  Analyzing $js_count JavaScript files (domain-scoped)..."

                # Redirect ONLY stderr to log file â€” stdout must stay free so the
                # Python script can write scan_results.json (it uses json.dump to
                # file, not stdout). Redirecting stdout was silently swallowing output.
                python3 production_js_scanner.py tmp/js_${domain}.txt "$domain" \
                  2>tmp/scanner_log_${domain}.txt || true

                # Show scanner progress in workflow log
                cat tmp/scanner_log_${domain}.txt || true

                if [[ -f scan_results.json ]]; then
                  mv scan_results.json tmp/scan_${domain}.json

                  finding_count=$(jq '.findings | length' tmp/scan_${domain}.json 2>/dev/null || echo "0")
                  critical=$(jq '[.findings[] | select(.severity=="CRITICAL")] | length' tmp/scan_${domain}.json 2>/dev/null || echo "0")
                  high=$(jq '[.findings[] | select(.severity=="HIGH")] | length' tmp/scan_${domain}.json 2>/dev/null || echo "0")
                  analyzed=$(jq '.stats.analyzed' tmp/scan_${domain}.json 2>/dev/null || echo "0")
                  skipped_tp=$(jq '.stats.skipped_third_party' tmp/scan_${domain}.json 2>/dev/null || echo "0")
                  skipped_html=$(jq '.stats.skipped_html' tmp/scan_${domain}.json 2>/dev/null || echo "0")

                  total_findings=$((total_findings + finding_count))
                  critical_findings=$((critical_findings + critical))
                  high_findings=$((high_findings + high))
                  js_analyzed=$((js_analyzed + analyzed))

                  cat > "$report" << EOF
          # ğŸ§  JS Security Analysis: $domain

          **Scan Date:** $(date '+%Y-%m-%d %H:%M UTC')
          **Run:** #${{ github.run_number }}
          **Intelligence:** ${GROQ_API_KEY:+Groq LLM Active}${GROQ_API_KEY:-Pattern-Only Mode}

          ---

          ## ğŸ“Š Scan Statistics

          | Metric | Value |
          |--------|------:|
          | JS Files Discovered | $js_count |
          | Files Analyzed | $analyzed |
          | Skipped (Third-Party) | $skipped_tp |
          | Skipped (HTML Response) | $skipped_html |
          | Total Findings | **$finding_count** |
          | Critical Issues | **$critical** |
          | High Severity | **$high** |

          EOF

                  if [[ "$finding_count" -gt 0 ]]; then
                    echo "" >> "$report"
                    echo "## ğŸš¨ Security Findings" >> "$report"
                    echo "" >> "$report"

                    for severity in CRITICAL HIGH MEDIUM; do
                      count=$(jq "[.findings[] | select(.severity==\"$severity\")] | length" tmp/scan_${domain}.json 2>/dev/null || echo "0")
                      if [[ "$count" -gt 0 ]]; then
                        echo "### ${severity} Severity ($count)" >> "$report"
                        echo "" >> "$report"

                        jq -r ".findings[] | select(.severity==\"$severity\") | \"
          #### \(.type | gsub(\"_\"; \" \") | ascii_upcase)

          **Severity:** \(.severity)
          **Risk:** \(.risk)
          **File:** \`\(.url)\`
          **Source Domain:** \`\(.source_domain // \"unknown\")\`
          \(if .line then \"**Line:** ~\(.line)\" else \"\" end)
          **Confidence:** \(.confidence)

          **Evidence:**
          \`\`\`javascript
          \(.evidence // .context)
          \`\`\`

          **âš¡ Action Required:**
          > \(.recommendation)

          ---
          \"" tmp/scan_${domain}.json >> "$report" 2>/dev/null
                      fi
                    done
                  else
                    echo "" >> "$report"
                    echo "## âœ… No Security Issues Detected" >> "$report"
                    echo "" >> "$report"
                    echo "All analyzed JavaScript files passed security checks." >> "$report"
                  fi

                  echo "âœ… Findings: $finding_count ($critical critical, $high high) | Skipped HTML: $skipped_html | Skipped 3rd-party: $skipped_tp"
                else
                  echo "âš ï¸ Scanner failed"
                fi
              else
                echo "âš ï¸ No same-domain JavaScript found after filtering"
                cat > "$report" << EOF
          # ğŸ§  JS Security Analysis: $domain

          **Scan Date:** $(date '+%Y-%m-%d %H:%M UTC')
          **Run:** #${{ github.run_number }}

          ## â„¹ï¸ No JavaScript Found

          No same-domain JavaScript files were discovered for this target.
          All discovered JS was filtered as third-party or non-JS content.
          EOF
              fi
            else
              echo "âš ï¸ Discovery failed"
            fi

            sleep 1

          done < tmp/selected.txt

          echo ""
          echo "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•"
          echo "ğŸ“Š SCAN COMPLETE"
          echo "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•"
          echo "Domains:         $domains_scanned"
          echo "JS Files:        $js_analyzed"
          echo "Total Findings:  $total_findings"
          echo "  Critical:      $critical_findings"
          echo "  High:          $high_findings"
          echo "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•"

          cat > stats/scan_${{ github.run_number }}.json << STATS_EOF
          {
            "run": ${{ github.run_number }},
            "date": "$(date -u +%Y-%m-%dT%H:%M:%SZ)",
            "domains": $domains_scanned,
            "js_files": $js_analyzed,
            "findings": {
              "total": $total_findings,
              "critical": $critical_findings,
              "high": $high_findings
            }
          }
          STATS_EOF

          echo "DOMAINS_SCANNED=$domains_scanned"   >> $GITHUB_ENV
          echo "JS_ANALYZED=$js_analyzed"           >> $GITHUB_ENV
          echo "TOTAL_FINDINGS=$total_findings"     >> $GITHUB_ENV
          echo "CRITICAL_FINDINGS=$critical_findings" >> $GITHUB_ENV
          echo "HIGH_FINDINGS=$high_findings"       >> $GITHUB_ENV

          [[ $total_findings  -gt 0 ]] && echo "HAS_FINDINGS=true"  >> $GITHUB_ENV || echo "HAS_FINDINGS=false"  >> $GITHUB_ENV
          [[ $critical_findings -gt 0 ]] && echo "HAS_CRITICAL=true" >> $GITHUB_ENV || echo "HAS_CRITICAL=false" >> $GITHUB_ENV

      - name: Generate Executive Summary
        if: always()
        run: |
          mkdir -p reports

          cat > reports/EXECUTIVE_SUMMARY.md << 'SUMMARY_EOF'
          # ğŸ§  JS Security Scanner - Executive Summary

          **Scan Date:** $(date '+%Y-%m-%d %H:%M UTC')
          **Run:** #${{ github.run_number }}
          **Intelligence:** ${GROQ_API_KEY:+âœ… AI-Powered}${GROQ_API_KEY:-âš ï¸ Pattern-Only}

          ---

          ## ğŸ“Š Key Metrics

          | Metric | Value | Status |
          |--------|------:|:------:|
          | Domains Scanned | ${DOMAINS_SCANNED:-0} | âœ… |
          | JS Files Analyzed | ${JS_ANALYZED:-0} | âœ… |
          | **Total Findings** | **${TOTAL_FINDINGS:-0}** | $(if [[ "${TOTAL_FINDINGS:-0}" -gt 0 ]]; then echo "âš ï¸"; else echo "âœ…"; fi) |
          | Critical Issues | ${CRITICAL_FINDINGS:-0} | $(if [[ "${CRITICAL_FINDINGS:-0}" -gt 0 ]]; then echo "ğŸš¨"; else echo "âœ…"; fi) |
          | High Severity | ${HIGH_FINDINGS:-0} | $(if [[ "${HIGH_FINDINGS:-0}" -gt 0 ]]; then echo "âš ï¸"; else echo "âœ…"; fi) |

          ## ğŸ¯ Scanned Domains

          $(cat tmp/selected.txt 2>/dev/null | nl -w2 -s'. ' || echo "None")

          ## ğŸ” Assessment

          $(if [[ "${CRITICAL_FINDINGS:-0}" -gt 0 ]]; then
            echo "### ğŸš¨ CRITICAL ALERT"
            echo ""
            echo "${CRITICAL_FINDINGS} critical security issue(s) detected requiring **immediate action**."
            echo ""
            echo "**Recommended Actions:**"
            echo "1. Review detailed reports immediately"
            echo "2. Rotate any exposed credentials"
            echo "3. Check access logs for unauthorized use"
            echo "4. Implement remediation steps"
          elif [[ "${HIGH_FINDINGS:-0}" -gt 0 ]]; then
            echo "### âš ï¸ HIGH PRIORITY"
            echo ""
            echo "${HIGH_FINDINGS} high-severity issue(s) detected requiring **prompt attention**."
            echo ""
            echo "**Recommended Actions:**"
            echo "1. Review findings within 24 hours"
            echo "2. Assess impact and exposure"
            echo "3. Plan remediation timeline"
          elif [[ "${TOTAL_FINDINGS:-0}" -gt 0 ]]; then
            echo "### â„¹ï¸ FINDINGS DETECTED"
            echo ""
            echo "${TOTAL_FINDINGS} medium/low severity issue(s) detected."
            echo ""
            echo "**Recommended Actions:**"
            echo "1. Review at next scheduled security review"
            echo "2. Add to remediation backlog"
          else
            echo "### âœ… ALL CLEAR"
            echo ""
            echo "No security issues detected in analyzed JavaScript files."
            echo ""
            echo "**Next Steps:**"
            echo "- Continue regular scanning"
            echo "- Monitor for new deployments"
          fi)

          ## ğŸ“ Detailed Reports

          $(ls reports/*_report.md 2>/dev/null | sed 's/reports\//- /' || echo "- No domain reports")

          ---

          **Repository:** ${{ github.repository }}
          **Workflow:** https://github.com/${{ github.repository }}/actions/runs/${{ github.run_id }}
          SUMMARY_EOF

      - name: Upload Results
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: security-scan-${{ github.run_number }}
          path: |
            reports/
            stats/
            tmp/*.json
          retention-days: 30

      - name: Email Security Report
        if: always() && env.DOMAINS_SCANNED != '0'
        continue-on-error: true
        uses: dawidd6/action-send-mail@v3
        with:
          server_address: smtp.gmail.com
          server_port: 465
          username: ${{ secrets.EMAIL_USERNAME }}
          password: ${{ secrets.EMAIL_PASSWORD }}
          subject: "${{ env.HAS_CRITICAL == 'true' && 'ğŸš¨ CRITICAL' || env.HAS_FINDINGS == 'true' && 'âš ï¸ FINDINGS' || 'âœ… CLEAN' }} | JS Security Scan #${{ github.run_number }}"
          to: ${{ secrets.EMAIL_USERNAME }}
          from: Security Scanner <security@scanner.local>
          body: |
            ğŸ§  PRODUCTION JS SECURITY SCAN REPORT
            â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

            ğŸ“… $(date '+%Y-%m-%d %H:%M UTC')
            ğŸ”„ Run #${{ github.run_number }}

            ğŸ“Š METRICS
            â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
            â€¢ Domains:         ${{ env.DOMAINS_SCANNED }}
            â€¢ JS Files:        ${{ env.JS_ANALYZED }}
            â€¢ Total Findings:  ${{ env.TOTAL_FINDINGS }}
            â€¢ ğŸ”¥ Critical:     ${{ env.CRITICAL_FINDINGS }}
            â€¢ âš ï¸  High:        ${{ env.HIGH_FINDINGS }}

            ${{ env.HAS_CRITICAL == 'true' && 'ğŸš¨ğŸš¨ğŸš¨ CRITICAL ISSUES - IMMEDIATE ACTION REQUIRED ğŸš¨ğŸš¨ğŸš¨' || env.HAS_FINDINGS == 'true' && 'âš ï¸ SECURITY ISSUES DETECTED - REVIEW REQUIRED' || 'âœ… NO ISSUES DETECTED' }}

            ğŸ“ ATTACHMENTS
            â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
            â€¢ EXECUTIVE_SUMMARY.md - Overview
            â€¢ *_report.md - Detailed findings per domain

            ğŸ”— Full results:
            https://github.com/${{ github.repository }}/actions/runs/${{ github.run_id }}

            â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
            Powered by AI + Pattern Analysis | ${{ github.repository }}
          attachments: reports/*
          priority: ${{ env.HAS_CRITICAL == 'true' && 'high' || 'normal' }}

      - name: Cleanup
        if: always()
        run: rm -rf tmp/*.txt tmp/*.js 2>/dev/null || true
