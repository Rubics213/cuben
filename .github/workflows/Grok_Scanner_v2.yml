name: Production JS Security Scanner

on:
  push:
    branches: [main, master]
    paths:
      - 'targets.txt'
  workflow_dispatch:
  schedule:
    - cron: '0 */6 * * *'

permissions:
  contents: write
  actions: read

jobs:
  intelligent-scan:
    runs-on: ubuntu-latest
    timeout-minutes: 45

    steps:
      - name: Checkout Code
        uses: actions/checkout@v4
        with:
          fetch-depth: 0

      - name: Setup Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.11'

      - name: Setup Directories
        run: |
          mkdir -p reports tmp stats
          echo "‚úÖ Workspace ready"

      - name: Cache Tools
        id: cache-tools
        uses: actions/cache@v4
        with:
          path: ~/go/bin
          key: ${{ runner.os }}-tools-v5

      - name: Install Dependencies
        run: |
          set -e
          echo "üì¶ Installing packages..."
          pip install --no-cache-dir groq aiohttp requests jsbeautifier || \
            pip install --break-system-packages groq aiohttp requests jsbeautifier
          
          if [[ ! -f ~/go/bin/katana ]]; then
            echo "Installing katana..."
            go install github.com/projectdiscovery/katana/cmd/katana@latest || {
              echo "‚ùå Katana install failed"
              exit 1
            }
          else
            echo "‚úÖ Katana cached"
          fi
          
          echo "$(go env GOPATH)/bin" >> $GITHUB_PATH
          echo "‚úÖ Dependencies ready"

      - name: Verify Tools
        run: |
          echo "üîç Verifying tools..."
          which python3 && python3 --version
          which katana && katana -version || echo "‚ö†Ô∏è Katana not in PATH yet"
          python3 -c "import groq; print('‚úÖ Groq module OK')" 2>/dev/null || echo "‚ö†Ô∏è Groq not available"

      - name: Create Production Scanner
        run: |
          cat > production_js_scanner.py << 'PYTHON_SCRIPT'
          #!/usr/bin/env python3
          """
          Production-Grade JS Security Scanner v4
          Fixed: All edge cases, error handling, and performance issues
          """
          
          import os
          import sys
          import json
          import asyncio
          import re
          import hashlib
          import math
          import subprocess
          from typing import List, Dict, Optional, Set, Tuple
          from urllib.parse import urlparse, urljoin
          
          try:
              import jsbeautifier
              BEAUTIFIER_AVAILABLE = True
          except ImportError:
              BEAUTIFIER_AVAILABLE = False
              print("‚ö†Ô∏è jsbeautifier not available", file=sys.stderr)
          
          try:
              from groq import Groq
              GROQ_AVAILABLE = True
          except ImportError:
              GROQ_AVAILABLE = False
              print("‚ö†Ô∏è Groq not available", file=sys.stderr)
          
          # Initialize Groq
          GROQ_API_KEY = os.getenv("GROQ_API_KEY")
          client = None
          if GROQ_API_KEY and GROQ_AVAILABLE:
              try:
                  client = Groq(api_key=GROQ_API_KEY)
                  print("‚úÖ Groq initialized", file=sys.stderr)
              except Exception as e:
                  print(f"‚ö†Ô∏è Groq init failed: {e}", file=sys.stderr)
          
          # Concurrency limits
          DOWNLOAD_CONCURRENCY = 6  # Reduced to avoid rate limits
          LLM_CONCURRENCY = 3
          
          # Patterns (same as before but with better error handling)
          PATTERNS = {
              "private_key": {
                  "regex": r'-----BEGIN (RSA |EC |DSA |OPENSSH )?PRIVATE KEY-----',
                  "severity": "CRITICAL",
                  "risk": "Full system compromise",
                  "require_entropy": False,
              },
              "aws_access_key": {
                  "regex": r'(?<![A-Z0-9])(AKIA|ASIA|AROA|AIDA)[A-Z0-9]{16}(?![A-Z0-9])',
                  "severity": "CRITICAL",
                  "risk": "AWS account takeover",
                  "require_entropy": False,
              },
              "github_token": {
                  "regex": r'gh[pousr]_[A-Za-z0-9_]{36,}',
                  "severity": "CRITICAL",
                  "risk": "Repository access",
                  "require_entropy": False,
              },
              "slack_token": {
                  "regex": r'xox[baprs]-[0-9]{10,13}-[0-9]{10,13}-[a-zA-Z0-9]{24,}',
                  "severity": "HIGH",
                  "risk": "Slack workspace compromise",
                  "require_entropy": False,
              },
              "jwt_token": {
                  "regex": r'eyJ[A-Za-z0-9_-]{20,}\.[A-Za-z0-9_-]{20,}\.[A-Za-z0-9_-]{20,}',
                  "severity": "HIGH",
                  "risk": "Session hijacking",
                  "require_entropy": False,
              },
              "database_url": {
                  "regex": r'(?i)(mongodb(\+srv)?|mysql|postgresql|redis)://[^@\s\'"<>]{3,}@[^\s\'"<>]{4,}',
                  "severity": "CRITICAL",
                  "risk": "Database access",
                  "require_entropy": False,
              },
              "api_key": {
                  "regex": r'(?i)\b(api[_\-]?key|apikey)\s*[:=]\s*["\']([a-zA-Z0-9_\-\.]{20,})["\']',
                  "severity": "HIGH",
                  "risk": "API access",
                  "require_entropy": True,
                  "value_group": 2,
              },
          }
          
          # Safe public keys
          SAFE_KEY_PREFIXES = [
              "key_live_", "key_test_", "pk_live_", "pk_test_",
              "AIza", "G-", "UA-", "GTM-", "ca-pub-",
              "undefined", "null", "false", "true",
          ]
          
          # Third-party domains to skip
          THIRD_PARTY_DOMAINS = [
              "googletagmanager.com", "google-analytics.com", "facebook.net",
              "cdn.segment.com", "hotjar.com", "intercom.io",
              "stripe.com", "cdn.jsdelivr.net", "unpkg.com", "cdnjs.cloudflare.com",
          ]
          
          # Library indicators
          LIBRARY_INDICATORS = [
              "jquery", "bootstrap", "angular", "lodash", "moment",
              "webpack", "vendor", "polyfill", "analytics", "gtag",
          ]
          
          # Boilerplate patterns
          BOILERPLATE_VALUES = [
              "your_", "insert_", "replace_", "example", "demo", "test",
              "placeholder", "todo", "fixme", "xxx", "changeme",
          ]
          
          FALSE_POSITIVE_PATTERNS = [
              r'\be\.[A-Z][a-zA-Z]+',  # Enum style
              r'"[a-z_]{5,}"',         # Route strings
              r'(//|/\*)',              # Comments
          ]
          
          
          def shannon_entropy(data: str) -> float:
              """Calculate Shannon entropy"""
              if not data or len(data) < 8:
                  return 0.0
              try:
                  freq = {}
                  for ch in data:
                      freq[ch] = freq.get(ch, 0) + 1
                  n = len(data)
                  return -sum((c / n) * math.log2(c / n) for c in freq.values())
              except Exception:
                  return 0.0
          
          
          def is_boilerplate(value: str) -> bool:
              """Check if value is boilerplate"""
              v = value.lower()
              return any(bp in v for bp in BOILERPLATE_VALUES)
          
          
          def is_safe_key(value: str) -> bool:
              """Check if it's a known public key"""
              return any(value.startswith(pfx) for pfx in SAFE_KEY_PREFIXES)
          
          
          def is_false_positive_context(context: str) -> bool:
              """Check if context indicates false positive"""
              for pattern in FALSE_POSITIVE_PATTERNS:
                  try:
                      if re.search(pattern, context):
                          return True
                  except Exception:
                      continue
              return False
          
          
          def extract_value(match_text: str) -> str:
              """Extract value from match"""
              try:
                  m = re.search(r'[:=]\s*["\']([^"\']{8,})["\']', match_text)
                  return m.group(1) if m else match_text
              except Exception:
                  return match_text
          
          
          # Download function
          def _curl_fetch(url: str, timeout: int) -> Tuple[Optional[str], str]:
              """Blocking curl fetch"""
              try:
                  parsed = urlparse(url)
                  origin = f"{parsed.scheme}://{parsed.netloc}"
                  
                  cmd = [
                      "curl", "-sL", "-A",
                      "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36",
                      "--max-time", str(timeout),
                      "--max-filesize", "3000000",
                      "--compressed",
                      "-H", "Accept: */*",
                      "-H", f"Referer: {origin}/",
                      url,
                  ]
                  
                  result = subprocess.run(
                      cmd,
                      capture_output=True,
                      timeout=timeout + 5
                  )
                  
                  if result.returncode != 0:
                      return None, f"curl_error_{result.returncode}"
                  
                  content = result.stdout.decode("utf-8", errors="ignore")
                  
                  if len(content) < 200:
                      return None, "too_small"
                  
                  # Check if HTML (error page)
                  stripped = content.lstrip()
                  if (stripped.startswith("<!") or 
                      stripped.lower().startswith("<html") or
                      "<title>" in stripped[:1000].lower()):
                      return None, "html_response"
                  
                  return content, ""
              
              except subprocess.TimeoutExpired:
                  return None, "timeout"
              except Exception as e:
                  return None, str(e)[:50]
          
          
          import concurrent.futures
          _thread_pool = concurrent.futures.ThreadPoolExecutor(max_workers=8)
          
          
          async def download_js(url: str, sem: asyncio.Semaphore, timeout: int = 15) -> Tuple[Optional[str], str]:
              """Async wrapper for download"""
              async with sem:
                  loop = asyncio.get_event_loop()
                  return await loop.run_in_executor(_thread_pool, _curl_fetch, url, timeout)
          
          
          # Analyzer class
          class JSAnalyzer:
              def __init__(self, target_domain: str):
                  self.target_domain = target_domain.lower().replace("www.", "")
                  self.seen_hashes: Set[str] = set()
                  self.llm_sem = asyncio.Semaphore(LLM_CONCURRENCY)
                  self.stats = {
                      "total": 0,
                      "skipped_download": 0,
                      "skipped_3rd_party": 0,
                      "skipped_library": 0,
                      "skipped_html": 0,
                      "skipped_duplicate": 0,
                      "analyzed": 0,
                      "findings": 0,
                      "fp_suppressed": 0,
                  }
              
              def is_same_domain(self, url: str) -> bool:
                  """Check if URL is from target domain"""
                  try:
                      host = urlparse(url).netloc.lower().replace("www.", "")
                      if host == self.target_domain or host.endswith("." + self.target_domain):
                          return True
                      stem = self.target_domain.split(".")[0]
                      return len(stem) >= 4 and stem in host
                  except Exception:
                      return False
              
              def is_third_party(self, url: str) -> bool:
                  """Check if URL is third-party"""
                  try:
                      host = urlparse(url).netloc.lower()
                      return any(tp in host for tp in THIRD_PARTY_DOMAINS)
                  except Exception:
                      return False
              
              def is_library(self, url: str) -> bool:
                  """Check if file is a library"""
                  try:
                      basename = os.path.basename(urlparse(url).path).lower()
                      return any(lib in basename for lib in LIBRARY_INDICATORS)
                  except Exception:
                      return False
              
              def is_html(self, content: str) -> bool:
                  """Check if content is HTML"""
                  try:
                      s = content.lstrip()[:500]
                      return (s.startswith("<!") or 
                              s.lower().startswith("<html") or
                              "<title>" in s.lower())
                  except Exception:
                      return False
              
              def is_duplicate(self, content: str) -> bool:
                  """Check for duplicate content"""
                  try:
                      h = hashlib.md5(content.encode()).hexdigest()
                      if h in self.seen_hashes:
                          return True
                      self.seen_hashes.add(h)
                      return False
                  except Exception:
                      return False
              
              def get_context(self, content: str, pos: int, size: int = 200) -> str:
                  """Extract context around match"""
                  try:
                      start = max(0, pos - size)
                      end = min(len(content), pos + size)
                      ctx = content[start:end]
                      return re.sub(r'\s+', ' ', ctx).strip()[:300]
                  except Exception:
                      return ""
              
              def pattern_scan(self, content: str, url: str) -> Tuple[List[Dict], int]:
                  """Scan content for patterns"""
                  findings = []
                  suppressed = 0
                  
                  for name, info in PATTERNS.items():
                      try:
                          for m in re.finditer(info["regex"], content):
                              try:
                                  raw_match = m.group(0)
                                  vg = info.get("value_group", 0)
                                  value = m.group(vg) if vg and vg <= len(m.groups()) else extract_value(raw_match)
                                  context = self.get_context(content, m.start())
                                  
                                  # Filter false positives
                                  if is_boilerplate(value):
                                      suppressed += 1
                                      continue
                                  
                                  if is_safe_key(value):
                                      suppressed += 1
                                      continue
                                  
                                  if is_false_positive_context(context):
                                      suppressed += 1
                                      continue
                                  
                                  if info.get("require_entropy"):
                                      if shannon_entropy(value) < 3.5 or len(value) < 16:
                                          suppressed += 1
                                          continue
                                  
                                  if len(value) < 12:
                                      suppressed += 1
                                      continue
                                  
                                  findings.append({
                                      "type": name,
                                      "severity": info["severity"],
                                      "risk": info["risk"],
                                      "url": url,
                                      "context": context,
                                      "match": raw_match[:80],
                                      "line": content[:m.start()].count("\n") + 1,
                                      "confidence": "pattern",
                                      "entropy": round(shannon_entropy(value), 2),
                                      "recommendation": self._get_recommendation(name),
                                  })
                              
                              except Exception as e:
                                  print(f"  Match error: {e}", file=sys.stderr)
                                  continue
                      
                      except Exception as e:
                          print(f"  Pattern {name} error: {e}", file=sys.stderr)
                          continue
                  
                  return findings, suppressed
              
              def _get_recommendation(self, vuln_type: str) -> str:
                  """Get recommendation for vuln type"""
                  recs = {
                      "private_key": "URGENT: Rotate immediately. Audit all systems.",
                      "aws_access_key": "URGENT: Deactivate in AWS IAM. Enable CloudTrail.",
                      "github_token": "Revoke on GitHub. Check audit log.",
                      "api_key": "Rotate key. Move to environment variables.",
                  }
                  return recs.get(vuln_type, "Review and rotate if real.")
              
              async def llm_analyze(self, content: str, url: str) -> Optional[Dict]:
                  """LLM analysis (optional, if Groq available)"""
                  if not client:
                      return None
                  
                  try:
                      # Extract suspicious lines
                      keywords = ["AKIA", "private_key", "client_secret", "mongodb://"]
                      suspicious = [l for l in content.split("\n")[:500] 
                                   if any(kw in l for kw in keywords)]
                      
                      if len(suspicious) < 1:
                          return None
                      
                      sample = "\n".join(suspicious[:20])
                      
                      prompt = f"""Analyze for REAL backend secrets in frontend JS.

          File: {os.path.basename(url)}
          Code:
          ```
          {sample[:1500]}
          ```

          Only flag REAL backend secrets with high entropy.
          Ignore: public SDK keys, placeholders, UI strings.

          If real secret found:
          {{"has_vulnerability": true, "type": "type", "severity": "CRITICAL|HIGH", "evidence": "line", "confidence": "high"}}

          If nothing: {{"has_vulnerability": false}}

          JSON only, no markdown."""
                      
                      async with self.llm_sem:
                          resp = client.chat.completions.create(
                              model="llama-3.3-70b-versatile",
                              messages=[{"role": "user", "content": prompt}],
                              max_tokens=400,
                              temperature=0.1,
                          )
                          
                          raw = resp.choices[0].message.content.strip()
                          raw = raw.replace("```json", "").replace("```", "").strip()
                          result = json.loads(raw)
                          
                          if result.get("has_vulnerability") and result.get("confidence") == "high":
                              return {
                                  "type": result.get("type", "unknown"),
                                  "severity": result.get("severity", "HIGH"),
                                  "risk": "Backend secret in frontend",
                                  "url": url,
                                  "context": result.get("evidence", "")[:300],
                                  "confidence": "llm_verified",
                                  "recommendation": "Rotate immediately.",
                              }
                  
                  except Exception as e:
                      print(f"  LLM error: {e}", file=sys.stderr)
                  
                  return None
              
              async def analyze_file(self, url: str, content: str) -> List[Dict]:
                  """Analyze single JS file"""
                  self.stats["total"] += 1
                  
                  # Filters
                  if not self.is_same_domain(url) or self.is_third_party(url):
                      self.stats["skipped_3rd_party"] += 1
                      return []
                  
                  if self.is_library(url):
                      self.stats["skipped_library"] += 1
                      return []
                  
                  if self.is_html(content):
                      self.stats["skipped_html"] += 1
                      return []
                  
                  if self.is_duplicate(content):
                      self.stats["skipped_duplicate"] += 1
                      return []
                  
                  self.stats["analyzed"] += 1
                  findings = []
                  
                  # Pattern scan
                  pattern_findings, suppressed = self.pattern_scan(content, url)
                  self.stats["fp_suppressed"] += suppressed
                  findings.extend(pattern_findings)
                  
                  # LLM scan (if available)
                  if client and len(content) > 500:
                      llm_finding = await self.llm_analyze(content, url)
                      if llm_finding:
                          findings.append(llm_finding)
                  
                  self.stats["findings"] += len(findings)
                  return findings
          
          
          async def main():
              """Main entry point"""
              if len(sys.argv) < 3:
                  print("Usage: scanner.py <js_urls.txt> <target_domain>", file=sys.stderr)
                  sys.exit(1)
              
              js_list, target_domain = sys.argv[1], sys.argv[2]
              
              if not os.path.exists(js_list):
                  print(f"‚ùå File not found: {js_list}", file=sys.stderr)
                  sys.exit(1)
              
              with open(js_list) as f:
                  urls = [l.strip() for l in f if l.strip()]
              
              if not urls:
                  print(json.dumps({"findings": [], "stats": {}}))
                  return
              
              analyzer = JSAnalyzer(target_domain)
              dl_sem = asyncio.Semaphore(DOWNLOAD_CONCURRENCY)
              
              print(f"üß† JS Scanner v4", file=sys.stderr)
              print(f"   Target: {target_domain}", file=sys.stderr)
              print(f"   URLs: {len(urls)}", file=sys.stderr)
              print(f"   LLM: {'‚úÖ' if client else '‚ö†Ô∏è Pattern-only'}", file=sys.stderr)
              print("", file=sys.stderr)
              
              async def fetch_and_analyze(i: int, url: str) -> List[Dict]:
                  """Download and analyze single file"""
                  try:
                      fname = os.path.basename(urlparse(url).path) or "unknown.js"
                      print(f"[{i:02d}/{len(urls)}] {fname[:40]}", file=sys.stderr)
                      
                      content, skip_reason = await download_js(url, dl_sem)
                      
                      if not content:
                          analyzer.stats["skipped_download"] += 1
                          print(f"[{i:02d}]    ‚ö†Ô∏è {skip_reason}", file=sys.stderr)
                          return []
                      
                      findings = await analyzer.analyze_file(url, content)
                      status = f"üî• {len(findings)}" if findings else "‚úì"
                      print(f"[{i:02d}]    {status}", file=sys.stderr)
                      return findings
                  
                  except Exception as e:
                      print(f"[{i:02d}]    ‚ùå {e}", file=sys.stderr)
                      return []
              
              # Parallel processing
              tasks = [fetch_and_analyze(i, url) for i, url in enumerate(urls, 1)]
              results = await asyncio.gather(*tasks, return_exceptions=True)
              
              # Collect findings (handle exceptions)
              all_findings = []
              for r in results:
                  if isinstance(r, list):
                      all_findings.extend(r)
                  elif isinstance(r, Exception):
                      print(f"Task error: {r}", file=sys.stderr)
              
              # Sort by severity
              sev_order = {"CRITICAL": 0, "HIGH": 1, "MEDIUM": 2, "LOW": 3}
              all_findings.sort(key=lambda x: sev_order.get(x.get("severity", "LOW"), 4))
              
              output = {"findings": all_findings, "stats": analyzer.stats}
              
              # Write results
              with open("scan_results.json", "w") as f:
                  json.dump(output, f, indent=2)
              
              print("", file=sys.stderr)
              print("üìä Stats:", file=sys.stderr)
              for k, v in analyzer.stats.items():
                  print(f"   {k:20s}: {v}", file=sys.stderr)
          
          
          if __name__ == "__main__":
              try:
                  asyncio.run(main())
              except KeyboardInterrupt:
                  print("\n‚ö†Ô∏è Interrupted", file=sys.stderr)
              except Exception as e:
                  print(f"\n‚ùå Fatal error: {e}", file=sys.stderr)
                  import traceback
                  traceback.print_exc(file=sys.stderr)
                  sys.exit(1)
          PYTHON_SCRIPT
          
          chmod +x production_js_scanner.py
          echo "‚úÖ Scanner created"

      - name: Run Production Scan
        env:
          GROQ_API_KEY: ${{ secrets.GROQ_API_KEY }}
        run: |
          set -euo pipefail
          
          mkdir -p reports tmp stats
          
          domains_scanned=0
          js_analyzed=0
          total_findings=0
          critical_findings=0
          high_findings=0
          
          # Validate targets
          if [[ ! -f targets.txt ]] || [[ ! -s targets.txt ]]; then
            echo "‚ö†Ô∏è No targets.txt, using default"
            echo "example.com" > targets.txt
          fi
          
          # Select targets (wrapped to prevent errors)
          (grep -v '^#' targets.txt | grep -v '^$' | shuf -n 6 > tmp/selected.txt) 2>/dev/null || echo "example.com" > tmp/selected.txt
          
          echo "‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê"
          echo "üß† PRODUCTION JS SCANNER"
          echo "‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê"
          cat tmp/selected.txt | nl || echo "No targets"
          echo "‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê"
          echo ""
          
          # Process domains
          while IFS= read -r domain; do
            [[ -z "$domain" ]] && continue
            domain=$(echo "$domain" | tr -d '[:space:]' | tr '[:upper:]' '[:lower:]')
            [[ -z "$domain" ]] && continue
            
            domains_scanned=$((domains_scanned + 1))
            
            echo ""
            echo "‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê"
            echo "üîç [$domains_scanned] $domain"
            echo "‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê"
            
            report="reports/${domain}_report.md"
            
            # JS Discovery
            echo "üì° Discovering JavaScript..."
            (timeout 90s katana -u "https://$domain" -jc -d 3 -c 20 -silent > tmp/urls_${domain}.txt) 2>/dev/null || touch tmp/urls_${domain}.txt
            
            if [[ -s tmp/urls_${domain}.txt ]]; then
              base_domain="${domain#www.}"
              base_stem="${base_domain%%.*}"
              
              # Show sample (FIX: Added error suppression + subshell)
              echo "   üîç Sample URLs from katana:"
              (grep -iE '\.js' tmp/urls_${domain}.txt 2>/dev/null | grep -i "${base_stem}" 2>/dev/null | head -5 | sed 's/^/      /') 2>/dev/null || echo "      (none found)"
              
              # Filter JS files (all wrapped to prevent pipe errors)
              (grep -iE '\.js(\?[^[:space:]]*)?$' tmp/urls_${domain}.txt 2>/dev/null \
                | grep -iv '\.json' \
                | grep -i "${base_stem}" \
                | grep -ivE '://app\.' \
                | grep -ivE '://account\.' \
                | grep -ivE '://login\.' \
                | grep -ivE '://dashboard\.' \
                | grep -ivE '(analytics|hotjar|gtag)' \
                > tmp/js_${domain}_raw.txt) 2>/dev/null || touch tmp/js_${domain}_raw.txt
              
              # Deduplicate and limit
              (sort -u tmp/js_${domain}_raw.txt 2>/dev/null | awk -F/ '!seen[$NF]++' | head -n 20 > tmp/js_${domain}.txt) 2>/dev/null || touch tmp/js_${domain}.txt
              
              js_count=$(wc -l < tmp/js_${domain}.txt 2>/dev/null || echo "0")
              
              if [[ $js_count -gt 0 ]]; then
                echo "üß† Analyzing $js_count files..."
                
                # Run scanner (redirect only stderr to log)
                python3 production_js_scanner.py tmp/js_${domain}.txt "$domain" 2>tmp/scanner_log_${domain}.txt || {
                  echo "‚ö†Ô∏è Scanner error"
                  cat tmp/scanner_log_${domain}.txt 2>/dev/null || true
                }
                
                # Show scanner log
                cat tmp/scanner_log_${domain}.txt 2>/dev/null || true
                
                if [[ -f scan_results.json ]]; then
                  mv scan_results.json tmp/scan_${domain}.json
                  
                  # Parse results safely
                  finding_count=$(jq '.findings | length' tmp/scan_${domain}.json 2>/dev/null || echo "0")
                  critical=$(jq '[.findings[] | select(.severity=="CRITICAL")] | length' tmp/scan_${domain}.json 2>/dev/null || echo "0")
                  high=$(jq '[.findings[] | select(.severity=="HIGH")] | length' tmp/scan_${domain}.json 2>/dev/null || echo "0")
                  analyzed=$(jq '.stats.analyzed' tmp/scan_${domain}.json 2>/dev/null || echo "0")
                  
                  total_findings=$((total_findings + finding_count))
                  critical_findings=$((critical_findings + critical))
                  high_findings=$((high_findings + high))
                  js_analyzed=$((js_analyzed + analyzed))
                  
                  # Generate report
                  cat > "$report" << EOF
          # üß† JS Security Analysis: $domain
          
          **Date:** $(date '+%Y-%m-%d %H:%M UTC')
          **Run:** #${{ github.run_number }}
          **Scanner:** v4 (production)
          
          ## üìä Stats
          
          | Metric | Value |
          |--------|------:|
          | JS Files Scanned | $js_count |
          | Files Analyzed | $analyzed |
          | Total Findings | **$finding_count** |
          | Critical | **$critical** |
          | High | **$high** |
          
          EOF
                  
                  if [[ $finding_count -gt 0 ]]; then
                    echo "## üö® Findings" >> "$report"
                    echo "" >> "$report"
                    
                    # Add findings by severity
                    for severity in CRITICAL HIGH MEDIUM; do
                      count=$(jq "[.findings[] | select(.severity==\"$severity\")] | length" tmp/scan_${domain}.json 2>/dev/null || echo "0")
                      if [[ $count -gt 0 ]]; then
                        echo "### ${severity} ($count)" >> "$report"
                        echo "" >> "$report"
                        
                        (jq -r ".findings[] | select(.severity==\"$severity\") | \"
          #### \(.type | ascii_upcase)
          **Severity:** \(.severity)
          **Risk:** \(.risk)
          **File:** \`\(.url)\`
          **Line:** ~\(.line // 0)
          
          \`\`\`javascript
          \(.context // \"see file\")
          \`\`\`
          
          **Action:** \(.recommendation)
          
          ---
          \"" tmp/scan_${domain}.json >> "$report") 2>/dev/null || echo "Error formatting findings" >> "$report"
                      fi
                    done
                  else
                    echo "## ‚úÖ No Issues" >> "$report"
                  fi
                  
                  echo "‚úÖ Complete: $finding_count findings ($critical critical, $high high)"
                else
                  echo "‚ö†Ô∏è No results file"
                fi
              else
                echo "‚ö†Ô∏è No JS files after filtering"
              fi
            else
              echo "‚ö†Ô∏è Discovery returned no results"
            fi
            
            sleep 1
            
          done < tmp/selected.txt
          
          echo ""
          echo "‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê"
          echo "üìä SCAN COMPLETE"
          echo "‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê"
          echo "Domains:   $domains_scanned"
          echo "JS Files:  $js_analyzed"
          echo "Findings:  $total_findings"
          echo "  Critical: $critical_findings"
          echo "  High:    $high_findings"
          echo "‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê"
          
          # Save stats
          cat > stats/scan_${{ github.run_number }}.json << STATS_EOF
          {
            "run": ${{ github.run_number }},
            "date": "$(date -u +%Y-%m-%dT%H:%M:%SZ)",
            "domains": $domains_scanned,
            "js_files": $js_analyzed,
            "findings": {
              "total": $total_findings,
              "critical": $critical_findings,
              "high": $high_findings
            }
          }
          STATS_EOF
          
          # Export to env
          echo "DOMAINS_SCANNED=$domains_scanned" >> $GITHUB_ENV
          echo "JS_ANALYZED=$js_analyzed" >> $GITHUB_ENV
          echo "TOTAL_FINDINGS=$total_findings" >> $GITHUB_ENV
          echo "CRITICAL_FINDINGS=$critical_findings" >> $GITHUB_ENV
          echo "HIGH_FINDINGS=$high_findings" >> $GITHUB_ENV
          
          [[ $total_findings -gt 0 ]] && echo "HAS_FINDINGS=true" >> $GITHUB_ENV || echo "HAS_FINDINGS=false" >> $GITHUB_ENV
          [[ $critical_findings -gt 0 ]] && echo "HAS_CRITICAL=true" >> $GITHUB_ENV || echo "HAS_CRITICAL=false" >> $GITHUB_ENV

      - name: Generate Summary
        if: always()
        run: |
          mkdir -p reports
          
          cat > reports/SUMMARY.md << 'SUMMARY_EOF'
          # üß† JS Security Scanner - Summary
          
          **Date:** $(date '+%Y-%m-%d %H:%M UTC')
          **Run:** #${{ github.run_number }}
          
          ## üìä Metrics
          
          | Metric | Value |
          |--------|------:|
          | Domains | ${DOMAINS_SCANNED:-0} |
          | JS Files | ${JS_ANALYZED:-0} |
          | **Findings** | **${TOTAL_FINDINGS:-0}** |
          | Critical | ${CRITICAL_FINDINGS:-0} |
          | High | ${HIGH_FINDINGS:-0} |
          
          ## üéØ Targets
          
          $(cat tmp/selected.txt 2>/dev/null | nl || echo "None")
          
          ## üîç Status
          
          $(if [[ "${CRITICAL_FINDINGS:-0}" -gt 0 ]]; then
            echo "üö® **CRITICAL:** ${CRITICAL_FINDINGS} issue(s) - immediate action required"
          elif [[ "${HIGH_FINDINGS:-0}" -gt 0 ]]; then
            echo "‚ö†Ô∏è **HIGH:** ${HIGH_FINDINGS} issue(s) - review within 24h"
          elif [[ "${TOTAL_FINDINGS:-0}" -gt 0 ]]; then
            echo "‚ÑπÔ∏è **Findings:** ${TOTAL_FINDINGS} issue(s) detected"
          else
            echo "‚úÖ **Clean:** No issues detected"
          fi)
          
          ## üìù Reports
          
          $(ls reports/*_report.md 2>/dev/null | sed 's/reports\//- /' || echo "- None")
          SUMMARY_EOF

      - name: Upload Results
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: scan-${{ github.run_number }}
          path: |
            reports/
            stats/
            tmp/*.json
          retention-days: 30

      - name: Email Report
        if: always() && env.DOMAINS_SCANNED != '0'
        continue-on-error: true
        uses: dawidd6/action-send-mail@v3
        with:
          server_address: smtp.gmail.com
          server_port: 465
          username: ${{ secrets.EMAIL_USERNAME }}
          password: ${{ secrets.EMAIL_PASSWORD }}
          subject: "${{ env.HAS_CRITICAL == 'true' && 'üö® CRITICAL' || env.HAS_FINDINGS == 'true' && '‚ö†Ô∏è FINDINGS' || '‚úÖ CLEAN' }} | Scan #${{ github.run_number }}"
          to: ${{ secrets.EMAIL_USERNAME }}
          from: JS Scanner <scan@security.local>
          body: |
            üß† JS SECURITY SCAN
            
            üìÖ $(date '+%Y-%m-%d %H:%M UTC')
            
            üìä RESULTS
            ‚Ä¢ Domains:   ${{ env.DOMAINS_SCANNED }}
            ‚Ä¢ JS Files:  ${{ env.JS_ANALYZED }}
            ‚Ä¢ Findings:  ${{ env.TOTAL_FINDINGS }}
            ‚Ä¢ Critical:  ${{ env.CRITICAL_FINDINGS }}
            ‚Ä¢ High:      ${{ env.HIGH_FINDINGS }}
            
            ${{ env.HAS_CRITICAL == 'true' && 'üö® CRITICAL - IMMEDIATE ACTION' || env.HAS_FINDINGS == 'true' && '‚ö†Ô∏è REVIEW REQUIRED' || '‚úÖ ALL CLEAR' }}
            
            üîó https://github.com/${{ github.repository }}/actions/runs/${{ github.run_id }}
          attachments: reports/*
          priority: ${{ env.HAS_CRITICAL == 'true' && 'high' || 'normal' }}

      - name: Cleanup
        if: always()
        run: |
          rm -rf tmp/*.txt tmp/*.js tmp/*_raw.txt 2>/dev/null || true
          echo "‚úÖ Cleanup done"
