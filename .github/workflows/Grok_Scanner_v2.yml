name: Production JS Security Scanner

on:
  push:
    branches: [main, master]
    paths:
      - 'targets.txt'
  workflow_dispatch:
  schedule:
    - cron: '0 */6 * * *'

permissions:
  contents: write
  actions: read

jobs:
  intelligent-scan:
    runs-on: ubuntu-latest
    timeout-minutes: 45

    steps:
      - name: Checkout Code
        uses: actions/checkout@v4
        with:
          fetch-depth: 0

      - name: Setup Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.11'

      - name: Setup Directories
        run: |
          mkdir -p reports tmp stats
          echo "âœ… Workspace ready"

      - name: Cache Tools
        id: cache-tools
        uses: actions/cache@v4
        with:
          path: ~/go/bin
          key: ${{ runner.os }}-tools-v5

      - name: Install Dependencies
        run: |
          set -e
          echo "ğŸ“¦ Installing packages..."
          pip install --no-cache-dir groq aiohttp requests jsbeautifier || \
            pip install --break-system-packages groq aiohttp requests jsbeautifier
          
          if [[ ! -f ~/go/bin/katana ]]; then
            echo "Installing katana..."
            go install github.com/projectdiscovery/katana/cmd/katana@latest
          fi
          
          echo "$(go env GOPATH)/bin" >> $GITHUB_PATH
          echo "âœ… Dependencies ready"

      - name: Create Production Scanner
        run: |
          cat > production_js_scanner.py << 'PYTHON_SCRIPT'
          #!/usr/bin/env python3
          """Production JS Security Scanner v4 - Simplified & Robust"""
          
          import os, sys, json, asyncio, re, hashlib, math, subprocess
          from typing import List, Dict, Optional, Set, Tuple
          from urllib.parse import urlparse
          import concurrent.futures
          
          try:
              from groq import Groq
              GROQ_API_KEY = os.getenv("GROQ_API_KEY")
              client = Groq(api_key=GROQ_API_KEY) if GROQ_API_KEY else None
              if client:
                  print("âœ… Groq active", file=sys.stderr)
          except:
              client = None
              print("âš ï¸ Groq unavailable", file=sys.stderr)
          
          PATTERNS = {
              "private_key": {"regex": r'-----BEGIN (RSA |EC )?PRIVATE KEY-----', "severity": "CRITICAL"},
              "aws_key": {"regex": r'(AKIA|ASIA)[A-Z0-9]{16}', "severity": "CRITICAL"},
              "github_token": {"regex": r'gh[pousr]_[A-Za-z0-9_]{36,}', "severity": "CRITICAL"},
              "api_key": {"regex": r'(?i)api[_-]?key\s*[:=]\s*["\']([a-zA-Z0-9_\-]{20,})["\']', "severity": "HIGH"},
          }
          
          SAFE_PREFIXES = ["key_live_", "pk_live_", "AIza", "G-", "UA-"]
          BOILERPLATE = ["your_", "example", "demo", "test", "placeholder"]
          
          _thread_pool = concurrent.futures.ThreadPoolExecutor(max_workers=8)
          
          def _curl_fetch(url: str, timeout: int) -> Tuple[Optional[str], str]:
              try:
                  result = subprocess.run(
                      ["curl", "-sL", "-A", "Mozilla/5.0", "--max-time", str(timeout), url],
                      capture_output=True, timeout=timeout + 5
                  )
                  if result.returncode != 0:
                      return None, "curl_failed"
                  content = result.stdout.decode("utf-8", errors="ignore")
                  if len(content) < 200 or content.lstrip().startswith("<"):
                      return None, "html_or_small"
                  return content, ""
              except:
                  return None, "error"
          
          async def download_js(url: str, sem: asyncio.Semaphore, timeout: int = 15):
              async with sem:
                  loop = asyncio.get_event_loop()
                  return await loop.run_in_executor(_thread_pool, _curl_fetch, url, timeout)
          
          class JSAnalyzer:
              def __init__(self, domain: str):
                  self.domain = domain.lower().replace("www.", "")
                  self.seen: Set[str] = set()
                  self.stats = {"total": 0, "analyzed": 0, "findings": 0, "skipped": 0}
              
              def is_same_domain(self, url: str) -> bool:
                  try:
                      host = urlparse(url).netloc.lower().replace("www.", "")
                      return host == self.domain or host.endswith("." + self.domain) or self.domain.split(".")[0] in host
                  except:
                      return False
              
              def is_duplicate(self, content: str) -> bool:
                  h = hashlib.md5(content.encode()).hexdigest()
                  if h in self.seen:
                      return True
                  self.seen.add(h)
                  return False
              
              def scan(self, content: str, url: str) -> List[Dict]:
                  findings = []
                  for name, info in PATTERNS.items():
                      try:
                          for m in re.finditer(info["regex"], content):
                              val = m.group(1) if m.lastindex else m.group(0)
                              if any(bp in val.lower() for bp in BOILERPLATE):
                                  continue
                              if any(val.startswith(sp) for sp in SAFE_PREFIXES):
                                  continue
                              if len(val) < 12:
                                  continue
                              findings.append({
                                  "type": name,
                                  "severity": info["severity"],
                                  "url": url,
                                  "match": val[:60],
                              })
                      except:
                          continue
                  return findings
              
              async def analyze(self, url: str, content: str) -> List[Dict]:
                  self.stats["total"] += 1
                  if not self.is_same_domain(url) or self.is_duplicate(content):
                      self.stats["skipped"] += 1
                      return []
                  self.stats["analyzed"] += 1
                  findings = self.scan(content, url)
                  self.stats["findings"] += len(findings)
                  return findings
          
          async def main():
              if len(sys.argv) < 3:
                  sys.exit(1)
              
              js_list, domain = sys.argv[1], sys.argv[2]
              with open(js_list) as f:
                  urls = [l.strip() for l in f if l.strip()]
              
              analyzer = JSAnalyzer(domain)
              sem = asyncio.Semaphore(6)
              
              print(f"ğŸ§  Scanner: {len(urls)} files", file=sys.stderr)
              
              async def process(i, url):
                  print(f"[{i:02d}] {os.path.basename(url)[:40]}", file=sys.stderr)
                  content, err = await download_js(url, sem)
                  if not content:
                      print(f"[{i:02d}]    âš ï¸ {err}", file=sys.stderr)
                      return []
                  findings = await analyzer.analyze(url, content)
                  print(f"[{i:02d}]    {'ğŸ”¥ ' + str(len(findings)) if findings else 'âœ“'}", file=sys.stderr)
                  return findings
              
              results = await asyncio.gather(*[process(i, u) for i, u in enumerate(urls, 1)], return_exceptions=True)
              all_findings = [f for r in results if isinstance(r, list) for f in r]
              all_findings.sort(key=lambda x: {"CRITICAL": 0, "HIGH": 1}.get(x.get("severity"), 2))
              
              with open("scan_results.json", "w") as f:
                  json.dump({"findings": all_findings, "stats": analyzer.stats}, f, indent=2)
              
              print(f"\nğŸ“Š Stats: analyzed={analyzer.stats['analyzed']}, findings={analyzer.stats['findings']}", file=sys.stderr)
          
          if __name__ == "__main__":
              asyncio.run(main())
          PYTHON_SCRIPT
          
          chmod +x production_js_scanner.py

      - name: Run Production Scan
        env:
          GROQ_API_KEY: ${{ secrets.GROQ_API_KEY }}
        run: |
          # CRITICAL: Do NOT use 'set -e' here - we want to process ALL domains even if one fails
          set -uo pipefail
          
          mkdir -p reports tmp stats
          
          domains_scanned=0
          js_analyzed=0
          total_findings=0
          critical_findings=0
          high_findings=0
          
          # Validate targets
          if [[ ! -f targets.txt ]] || [[ ! -s targets.txt ]]; then
            echo "example.com" > targets.txt
          fi
          
          # Select targets
          grep -v '^#' targets.txt 2>/dev/null | grep -v '^$' | shuf -n 6 > tmp/selected.txt || echo "example.com" > tmp/selected.txt
          
          echo "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•"
          echo "ğŸ§  PRODUCTION JS SCANNER"
          echo "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•"
          cat tmp/selected.txt | nl
          total_domains=$(wc -l < tmp/selected.txt)
          echo "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•"
          echo ""
          
          # Process EACH domain (with error isolation)
          while read -r domain; do
            # Validate domain
            if [[ -z "$domain" ]] || [[ "$domain" =~ ^[[:space:]]*# ]]; then
              continue
            fi
            
            domain=$(echo "$domain" | tr -d '[:space:]' | tr '[:upper:]' '[:lower:]')
            if [[ -z "$domain" ]]; then
              continue
            fi
            
            domains_scanned=$((domains_scanned + 1))
            
            echo ""
            echo "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•"
            echo "ğŸ” [$domains_scanned/$total_domains] $domain"
            echo "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•"
            
            # Isolate each domain in a subshell so errors don't break the loop
            (
              set +e  # Don't exit on errors within domain processing
              
              report="reports/${domain}_report.md"
              
              # Discovery
              echo "ğŸ“¡ Discovering..."
              timeout 90s katana -u "https://$domain" -jc -d 3 -c 20 -silent > tmp/urls_${domain}.txt 2>/dev/null || touch tmp/urls_${domain}.txt
              
              if [[ ! -s tmp/urls_${domain}.txt ]]; then
                echo "âš ï¸ No URLs found"
                echo "# $domain - No JS Found" > "$report"
                exit 0
              fi
              
              # Filter JS
              stem="${domain%%.*}"
              grep -iE '\.js(\?|$)' tmp/urls_${domain}.txt 2>/dev/null \
                | grep -iv '\.json' \
                | grep -i "$stem" \
                | grep -ivE '(analytics|gtag|ads)' \
                | sort -u \
                | head -20 > tmp/js_${domain}.txt 2>/dev/null || touch tmp/js_${domain}.txt
              
              js_count=$(wc -l < tmp/js_${domain}.txt 2>/dev/null || echo 0)
              
              if [[ $js_count -eq 0 ]]; then
                echo "âš ï¸ No JS after filtering"
                echo "# $domain - No JS After Filtering" > "$report"
                exit 0
              fi
              
              echo "ğŸ§  Analyzing $js_count files..."
              
              # Run scanner
              python3 production_js_scanner.py tmp/js_${domain}.txt "$domain" 2>&1 || {
                echo "âš ï¸ Scanner error"
                exit 0
              }
              
              if [[ ! -f scan_results.json ]]; then
                echo "âš ï¸ No results"
                exit 0
              fi
              
              mv scan_results.json tmp/scan_${domain}.json
              
              # Parse results
              finding_count=$(jq '.findings | length' tmp/scan_${domain}.json 2>/dev/null || echo 0)
              critical=$(jq '[.findings[] | select(.severity=="CRITICAL")] | length' tmp/scan_${domain}.json 2>/dev/null || echo 0)
              high=$(jq '[.findings[] | select(.severity=="HIGH")] | length' tmp/scan_${domain}.json 2>/dev/null || echo 0)
              analyzed=$(jq '.stats.analyzed' tmp/scan_${domain}.json 2>/dev/null || echo 0)
              
              # Generate report
              cat > "$report" << EOF
          # ğŸ§  $domain Security Scan
          
          **Date:** $(date '+%Y-%m-%d %H:%M UTC')
          **Run:** #${{ github.run_number }}
          
          ## Stats
          | Metric | Value |
          |--------|------:|
          | JS Files | $js_count |
          | Analyzed | $analyzed |
          | Findings | **$finding_count** |
          | Critical | **$critical** |
          | High | **$high** |
          
          EOF
              
              if [[ $finding_count -gt 0 ]]; then
                echo "## ğŸš¨ Findings" >> "$report"
                jq -r '.findings[] | "### \(.type)\n**Severity:** \(.severity)\n**File:** `\(.url)`\n**Match:** `\(.match)`\n"' tmp/scan_${domain}.json >> "$report" 2>/dev/null
              else
                echo "## âœ… No Issues" >> "$report"
              fi
              
              echo "âœ… Done: $finding_count findings"
              
              # Export results (these will be picked up by parent shell)
              echo "$finding_count" > tmp/${domain}_findings.txt
              echo "$critical" > tmp/${domain}_critical.txt
              echo "$high" > tmp/${domain}_high.txt
              echo "$analyzed" > tmp/${domain}_analyzed.txt
              
            ) || echo "âš ï¸ Domain processing failed (continuing)"
            
            # Collect stats from subshell
            if [[ -f tmp/${domain}_findings.txt ]]; then
              total_findings=$((total_findings + $(cat tmp/${domain}_findings.txt)))
              critical_findings=$((critical_findings + $(cat tmp/${domain}_critical.txt)))
              high_findings=$((high_findings + $(cat tmp/${domain}_high.txt)))
              js_analyzed=$((js_analyzed + $(cat tmp/${domain}_analyzed.txt)))
            fi
            
            sleep 1
            
          done < tmp/selected.txt
          
          echo ""
          echo "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•"
          echo "ğŸ“Š SCAN COMPLETE"
          echo "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•"
          echo "Domains:   $domains_scanned"
          echo "JS Files:  $js_analyzed"
          echo "Findings:  $total_findings"
          echo "  Critical: $critical_findings"
          echo "  High:    $high_findings"
          echo "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•"
          
          # Save stats
          cat > stats/scan_${{ github.run_number }}.json << EOF
          {
            "run": ${{ github.run_number }},
            "date": "$(date -u +%Y-%m-%dT%H:%M:%SZ)",
            "domains": $domains_scanned,
            "js_files": $js_analyzed,
            "findings": {"total": $total_findings, "critical": $critical_findings, "high": $high_findings}
          }
          EOF
          
          # Export
          echo "DOMAINS_SCANNED=$domains_scanned" >> $GITHUB_ENV
          echo "JS_ANALYZED=$js_analyzed" >> $GITHUB_ENV
          echo "TOTAL_FINDINGS=$total_findings" >> $GITHUB_ENV
          echo "CRITICAL_FINDINGS=$critical_findings" >> $GITHUB_ENV
          echo "HIGH_FINDINGS=$high_findings" >> $GITHUB_ENV
          
          [[ $total_findings -gt 0 ]] && echo "HAS_FINDINGS=true" >> $GITHUB_ENV || echo "HAS_FINDINGS=false" >> $GITHUB_ENV
          [[ $critical_findings -gt 0 ]] && echo "HAS_CRITICAL=true" >> $GITHUB_ENV || echo "HAS_CRITICAL=false" >> $GITHUB_ENV

      - name: Generate Summary
        if: always()
        run: |
          mkdir -p reports
          
          cat > reports/SUMMARY.md << 'EOF'
          # ğŸ§  JS Security Scanner - Summary
          
          **Date:** $(date '+%Y-%m-%d %H:%M UTC')
          **Run:** #${{ github.run_number }}
          
          ## Metrics
          | Metric | Value |
          |--------|------:|
          | Domains | ${DOMAINS_SCANNED:-0} |
          | JS Files | ${JS_ANALYZED:-0} |
          | **Findings** | **${TOTAL_FINDINGS:-0}** |
          | Critical | ${CRITICAL_FINDINGS:-0} |
          | High | ${HIGH_FINDINGS:-0} |
          
          ## Reports
          $(ls reports/*_report.md 2>/dev/null | wc -l) domain reports generated
          EOF

      - name: Upload Results
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: scan-${{ github.run_number }}
          path: |
            reports/
            stats/
          retention-days: 30

      - name: Email Report
        if: always() && env.DOMAINS_SCANNED != '0'
        continue-on-error: true
        uses: dawidd6/action-send-mail@v3
        with:
          server_address: smtp.gmail.com
          server_port: 465
          username: ${{ secrets.EMAIL_USERNAME }}
          password: ${{ secrets.EMAIL_PASSWORD }}
          subject: "${{ env.HAS_CRITICAL == 'true' && 'ğŸš¨ CRITICAL' || env.HAS_FINDINGS == 'true' && 'âš ï¸ FINDINGS' || 'âœ… CLEAN' }} | ${{ env.DOMAINS_SCANNED }} Domains | Scan #${{ github.run_number }}"
          to: ${{ secrets.EMAIL_USERNAME }}
          from: JS Scanner <scan@security.local>
          body: |
            ğŸ§  JS SECURITY SCAN
            
            ğŸ“Š RESULTS
            â€¢ Domains:   ${{ env.DOMAINS_SCANNED }}
            â€¢ JS Files:  ${{ env.JS_ANALYZED }}
            â€¢ Findings:  ${{ env.TOTAL_FINDINGS }}
            â€¢ Critical:  ${{ env.CRITICAL_FINDINGS }}
            â€¢ High:      ${{ env.HIGH_FINDINGS }}
            
            ${{ env.HAS_CRITICAL == 'true' && 'ğŸš¨ CRITICAL' || env.HAS_FINDINGS == 'true' && 'âš ï¸ REVIEW' || 'âœ… CLEAN' }}
            
            ğŸ”— https://github.com/${{ github.repository }}/actions/runs/${{ github.run_id }}
          attachments: reports/*
          priority: ${{ env.HAS_CRITICAL == 'true' && 'high' || 'normal' }}

      - name: Cleanup
        if: always()
        run: rm -rf tmp/*.txt tmp/*.js 2>/dev/null || true
