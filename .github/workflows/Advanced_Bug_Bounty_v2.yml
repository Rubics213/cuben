name: Advanced Bug Bounty Scanner

on:
  push:
    branches: [main]
  pull_request:
    branches: [main]
  workflow_dispatch:
  schedule:
    - cron: '13 1 * * *'
    - cron: '47 5 * * *'
    - cron: '29 9 * * *'
    - cron: '53 14 * * *'
    - cron: '11 18 * * *'

jobs:
  vulnerability-scan:
    runs-on: ubuntu-latest
    timeout-minutes: 180 # Increased timeout due to added tools and deeper scanning

    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Cache Go modules
        uses: actions/cache@v4
        with:
          path: ~/go
          key: ${{ runner.os }}-go-${{ hashFiles('go.mod') }} # Adjust if you have a go.mod, otherwise use a static key or hash other relevant files
          restore-keys: |
            ${{ runner.os }}-go-

      - name: Cache apt packages
        uses: actions/cache@v4
        with:
          path: /var/cache/apt/archives
          key: ${{ runner.os }}-apt-${{ hashFiles('.github/workflows/advanced_bug_bounty_scanner.yml') }} # Hash the workflow file for cache invalidation on dependency changes
          restore-keys: |
            ${{ runner.os }}-apt-

      - name: Install scanning dependencies
        run: |
          sudo apt-get update
          sudo apt-get install -y git python3 python3-pip nikto nmap unzip wget golang jq curl
          echo "GOPATH=$HOME/go" >> $GITHUB_ENV
          echo "PATH=$PATH:/usr/local/go/bin:$HOME/go/bin" >> $GITHUB_ENV

          # Install Go tools
          go install github.com/tomnomnom/assetfinder@latest
          go install github.com/projectdiscovery/nuclei/v3/cmd/nuclei@latest
          go install github.com/projectdiscovery/httpx/cmd/httpx@latest
          go install github.com/projectdiscovery/subfinder/v2/cmd/subfinder@latest
          go install github.com/lc/gau/v2/cmd/gau@latest
          go install github.com/projectdiscovery/katana/cmd/katana@latest
          go install github.com/sensepost/gowitness@latest # For web screenshots

          # Move Go binaries to a common PATH
          sudo mv ~/go/bin/{assetfinder,nuclei,httpx,subfinder,gau,katana,gowitness} /usr/local/bin/

          # Install github-endpoints (Python tool)
          pip3 install git+https://github.com/initstring/github-endpoints.git

          # Install Findomain
          curl -s https://api.github.com/repos/findomain/findomain/releases/latest \
            | grep "browser_download_url.*linux.zip" \
            | cut -d '"' -f 4 \
            | wget -i -
          unzip findomain-linux.zip
          chmod +x findomain
          sudo mv findomain /usr/local/bin/findomain

          # Update Nuclei templates
          nuclei -update-templates -silent

      - name: Log in to GitHub Container Registry
        run: echo ${{ secrets.GITHUB_TOKEN }} | docker login ghcr.io -u ${{ github.actor }} --password-stdin

      - name: Prepare Initial Target List (domains.txt / organizations.txt)
        id: prepare-targets
        run: |
          # Check for domains.txt or organizations.txt
          if [ ! -f ".github/workflows/domains.txt" ] && [ ! -f ".github/workflows/organizations.txt" ]; then
            echo "ERROR: Neither .github/workflows/domains.txt nor .github/workflows/organizations.txt file found!"
            exit 1
          fi

          # Create a temporary file for discovered domains
          discovered_domains_file="temp_discovered_domains.txt"
          touch "$discovered_domains_file"

          # Add existing domains from domains.txt (if it exists)
          if [ -f ".github/workflows/domains.txt" ]; then
            echo "Initial target domains from .github/workflows/domains.txt:"
            cat .github/workflows/domains.txt >> "$discovered_domains_file"
            sort -u "$discovered_domains_file" -o "$discovered_domains_file" # Deduplicate
            sed -i '/^$/d' "$discovered_domains_file" # Remove empty lines
            echo "Current merged list:"
            cat "$discovered_domains_file"
          fi

          # Dynamic Scope Discovery from organizations.txt (if it exists)
          if [ -f ".github/workflows/organizations.txt" ]; then
            echo "Starting dynamic discovery from .github/workflows/organizations.txt..."
            while IFS= read -r org; do
              org=$(echo "$org" | tr -d '\r\n')
              if [ -z "$org" ]; then continue; fi # Skip empty lines

              echo "üîç Discovering domains for organization: $org"

              # Use github-endpoints to find domains in public repos
              echo "Running github-endpoints for $org..."
              github-endpoints -o "$org" -m "domains" | httpx -silent >> "$discovered_domains_file" || true

              # Find domains via assetfinder (uses crt.sh, passive sources, etc.)
              echo "Running assetfinder for $org..."
              assetfinder --subs-only "$org" | httpx -silent >> "$discovered_domains_file" || true

            done < .github/workflows/organizations.txt
          fi

          # Final deduplication and cleanup
          sort -u "$discovered_domains_file" -o "$discovered_domains_file"
          sed -i '/^$/d' "$discovered_domains_file"

          echo "Final target domains for this run:"
          cat "$discovered_domains_file"

          # Overwrite domains.txt with the comprehensive list for the next step and future runs
          mv "$discovered_domains_file" .github/workflows/domains.txt

      - name: Detect subdomains (with multi-domain diffing)
        id: subdomain-detection
        run: |
          set -e
          mkdir -p subdomain_output old_subdomains
          new_subdomains_found=()

          # Ensure old_subdomains directory exists and potentially populate from artifact (if implemented)
          # For simplicity here, we assume if it's not present, it's the first run or a fresh clone.

          while IFS= read -r domain; do
            domain=$(echo "$domain" | tr -d '\r\n')
            if [ -z "$domain" ]; then continue; fi # Skip empty lines
            echo "üîé Processing $domain"
            output_file="subdomain_output/${domain//./_}_subdomains.txt"
            touch "$output_file" # Ensure file exists before appending

            # Run multiple subdomain enumeration tools
            assetfinder --subs-only "$domain" | httpx -silent >> "$output_file" || true
            subfinder -d "$domain" -silent >> "$output_file" || true
            findomain -t "$domain" --quiet >> "$output_file" || true
            gau "$domain" | grep "^\(http\|https\)" | httpx -silent >> "$output_file" || true # Passive URLs and subdomains

            sort -u "$output_file" -o "$output_file"
            sed -i '/^$/d' "$output_file" # Remove empty lines

            old_file="old_subdomains/${domain//./_}_old_subdomains.txt"
            if [ ! -f "$old_file" ]; then
              echo "üåü First scan for $domain ‚Äî storing results"
              cp "$output_file" "$old_file"
              new_subdomains_found+=("$domain")
            else
              if ! diff -q "$output_file" "$old_file" > /dev/null; then
                echo "üî• New subdomains detected for $domain!"
                new_subdomains_found+=("$domain")
                cp "$output_file" "$old_file"
              else
                echo "‚úÖ No new subdomains for $domain"
              fi
            fi
          done < .github/workflows/domains.txt # Read from the potentially updated domains.txt

          echo "NEW_DOMAINS=${new_subdomains_found[*]}" >> $GITHUB_ENV
          if [ "${#new_subdomains_found[@]}" -gt 0 ]; then
            echo "new_subdomains_detected=true" >> $GITHUB_ENV
          else
            echo "new_subdomains_detected=false" >> $GITHUB_ENV
          fi

      - name: Prepare ZAP plan
        run: |
          mkdir -p .github/zap
          if [ ! -f ".github/zap/plan.yml" ]; then
            printf "%s\n" \
              "plans:" \
              "  - name: \"Default Scan\"" \
              "    parameters:" \
              "      target: \"https://TARGET_DOMAIN\"" \
              "      rules:" \
              "        - \"scan_rules/automatic\"" \
              "    context:" \
              "      name: \"Default Context\"" \
              "      includePaths:" \
              "        - \"https://TARGET_DOMAIN/.*\"" \
              > .github/zap/plan.yml
          fi

      - name: Run ZAP Scan (Dynamic Targets)
        if: env.new_subdomains_detected == 'true'
        run: |
          for domain in $NEW_DOMAINS; do
            echo "üï∑Ô∏è Running ZAP scan for $domain"
            sed -i "s|TARGET_DOMAIN|$domain|g" .github/zap/plan.yml
            # Use '|| true' here so a ZAP internal error doesn't fail the entire workflow
            docker run --rm -v "$(pwd)":/zap/wrk -t ghcr.io/zaproxy/action-full-scan:latest -c .github/zap/plan.yml || echo "ZAP scan completed for $domain (may have errors)"
            sed -i "s|$domain|TARGET_DOMAIN|g" .github/zap/plan.yml # Revert change for next loop iteration
          done

      - name: Take Web Screenshots
        if: env.new_subdomains_detected == 'true'
        run: |
          mkdir -p screenshots
          for domain in $NEW_DOMAINS; do
            echo "üì∏ Taking screenshots for $domain"
            subdomains_file="subdomain_output/${domain//./_}_subdomains.txt"
            if [ -f "$subdomains_file" ]; then
              # Use gowitness to screenshot all discovered live subdomains
              gowitness file -f "$subdomains_file" -o "screenshots/${domain//./_}" || true
            else
              echo "No active subdomains file found for $domain to screenshot. Skipping."
            fi
          done

      - name: Run Nikto Scan
        if: env.new_subdomains_detected == 'true'
        run: |
          mkdir -p nikto-reports
          for domain in $NEW_DOMAINS; do
            echo "üî¨ Nikto scanning $domain"
            # Nikto can be noisy; using -nointeractive and allowing it to "fail" gracefully if no findings
            nikto -h "https://$domain" -Format htm -output "nikto-reports/nikto-${domain//./_}.html" -nointeractive || echo "Nikto scan completed for $domain"
          done

      - name: Run Nuclei Scan (Targeted and Deep)
        if: env.new_subdomains_detected == 'true'
        run: |
          mkdir -p nuclei-reports
          for domain in $NEW_DOMAINS; do
            echo "üí£ Nuclei scanning $domain"
            subdomains_file="subdomain_output/${domain//./_}_subdomains.txt"
            if [ -f "$subdomains_file" ]; then
              # Active crawling with katana to get more URLs
              echo "Crawling $domain with katana for deeper Nuclei scan..."
              katana -u "https://$domain" -silent -d 3 -jc -o "katana_urls_for_${domain//./_}.txt" || true
              
              # Combine subdomains and crawled URLs for Nuclei input
              cat "$subdomains_file" "katana_urls_for_${domain//./_}.txt" | sort -u > "all_targets_for_nuclei_${domain//./_}.txt"
              
              # Run Nuclei on the combined list of URLs/domains
              nuclei -l "all_targets_for_nuclei_${domain//./_}.txt" -severity critical,high,medium,low,info -j -o "nuclei-reports/nuclei-${domain//./_}.json" -timeout 60 -silent || echo "Nuclei scan completed for $domain"

              rm -f "katana_urls_for_${domain//./_}.txt" "all_targets_for_nuclei_${domain//./_}.txt" # Clean up temp files
            else
              echo "No active subdomains file found for $domain for Nuclei scan. Skipping."
            fi
          done

      - name: Generate Summary Reports
        if: env.new_subdomains_detected == 'true'
        run: |
          mkdir -p summaries
          echo "domain,severity,name,url" > summaries/nuclei-findings.csv
          for file in nuclei-reports/*.json; do
            if [ -s "$file" ]; then # Check if file is not empty
              jq -r '[.host, .info.severity, .info.name, .matched] | @csv' "$file" >> summaries/nuclei-findings.csv
            fi
          done

          echo "<html><body><h1>Scan Summary</h1>" > summaries/nuclei-summary.html
          echo "<h2>Nuclei Findings (Critical/High/Medium/Low/Info)</h2>" >> summaries/nuclei-summary.html
          echo "<table border='1'><tr><th>Domain</th><th>Severity</th><th>Name</th><th>URL</th></tr>" >> summaries/nuclei-summary.html
          if [ -s summaries/nuclei-findings.csv ]; then # Check if CSV is not empty
            tail -n +2 summaries/nuclei-findings.csv | while IFS=',' read -r domain severity name url; do
              echo "<tr><td>$domain</td><td>$severity</td><td>$name</td><td><a href='$url'>$url</a></td></tr>" >> summaries/nuclei-summary.html
            done
          else
            echo "<tr><td colspan='4'>No Nuclei findings reported.</td></tr>" >> summaries/nuclei-summary.html
          fi
          echo "</table>" >> summaries/nuclei-summary.html

          echo "<h2>Nikto Scan Reports (HTML)</h2>" >> summaries/nikto-summary.html
          echo "<ul>" >> summaries/nikto-summary.html
          for file in nikto-reports/*.html; do
              if [ -s "$file" ]; then
                  filename=$(basename "$file")
                  echo "<li><a href='../nikto-reports/$filename'>$filename</a></li>" >> summaries/nikto-summary.html
              fi
          done
          echo "</ul>" >> summaries/nikto-summary.html

          echo "<h2>Web Screenshots</h2>" >> summaries/screenshots-summary.html
          echo "<ul>" >> summaries/screenshots-summary.html
          find screenshots -name "*.png" | sed 's|^screenshots/||' | while read -r img_path; do
              echo "<li><a href='../screenshots/$img_path'>$img_path</a></li>" >> summaries/screenshots-summary.html
          done
          echo "</ul>" >> summaries/screenshots-summary.html


          echo "</body></html>" >> summaries/nuclei-summary.html


      - name: Upload Artifacts
        uses: actions/upload-artifact@v4
        if: env.new_subdomains_detected == 'true'
        with:
          name: scan-results-${{ github.run_id }}
          path: |
            summaries/*
            nikto-reports/*
            nuclei-reports/*
            screenshots/*
            subdomain_output/* # Keep the subdomain output for review

      - name: Create GitHub Issues for Critical/High
        if: env.new_subdomains_detected == 'true'
        env:
          GH_TOKEN: ${{ secrets.GITHUB_TOKEN }}
        run: |
          for file in nuclei-reports/*.json; do
            if [ -s "$file" ]; then # Check if file is not empty
              jq -c 'select(.info.severity == "critical" or .info.severity == "high")' "$file" | while read -r vuln; do
                title=$(echo "$vuln" | jq -r '.info.name')
                url=$(echo "$vuln" | jq -r '.matched')
                severity=$(echo "$vuln" | jq -r '.info.severity')
                template=$(echo "$vuln" | jq -r '.template') # Include template ID for better context
                body="**Severity**: $severity%0A**URL**: $url%0A**Template**: $template%0A**Found in Workflow Run**: https://github.com/${{ github.repository }}/actions/runs/${{ github.run_id }}"
                
                # Deduplication logic (basic - checks open issues with same title)
                # More advanced would involve a state file or querying specific labels/bodies
                existing_issue=$(gh issue list --search "is:open in:title \"$title [$severity]\"" --json number --jq '.[0].number' || true)
                if [ -z "$existing_issue" ]; then
                  echo "Creating GitHub issue for: $title [$severity]"
                  gh issue create --title "$title [$severity]" --body "$body" --label "bug,bounty,auto-created,${severity}"
                else
                  echo "Skipping duplicate issue for: $title [$severity] (Issue #$existing_issue already open)"
                fi
              done
            fi
          done

      - name: Notify Slack of Critical/High Findings
        if: env.new_subdomains_detected == 'true' && success() && secrets.SLACK_WEBHOOK_URL # Only if Slack URL is set
        run: |
          NOTIFICATIONS_SENT=false
          for file in nuclei-reports/*.json; do
            if [ -s "$file" ]; then # Check if file is not empty
              jq -c 'select(.info.severity == "critical" or .info.severity == "high")' "$file" | while read -r vuln; do
                domain=$(echo "$vuln" | jq -r '.host' | sed 's|https://||;s|http://||;s|/.*||') # Extract base domain
                title=$(echo "$vuln" | jq -r '.info.name')
                url=$(echo "$vuln" | jq -r '.matched')
                severity=$(echo "$vuln" | jq -r '.info.severity')
                template=$(echo "$vuln" | jq -r '.template') # Include template ID

                SLACK_MESSAGE="{\"text\":\"‚ö†Ô∏è *Critical/High vulnerability detected!*\n*Domain:* $domain\n*Severity:* $severity\n*Name:* $title\n*URL:* <$url|$url>\n*Template:* $template\n*Workflow Run:* https://github.com/${{ github.repository }}/actions/runs/${{ github.run_id }}\"}"

                echo "Sending Slack notification for: $title ($severity) on $domain"
                curl -X POST -H 'Content-type: application/json' \
                  --data "$SLACK_MESSAGE" \
                  "${{ secrets.SLACK_WEBHOOK_URL }}" || true # Use || true to prevent job failure if Slack fails
                NOTIFICATIONS_SENT=true
              done
            fi
          done

          if [ "$NOTIFICATIONS_SENT" = false ]; then
            echo "No Critical/High findings to notify Slack about."
          fi
